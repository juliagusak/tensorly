{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.finfo(torch.float32).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import peach as p\n",
    "\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "\n",
    "from tensorly.decomposition import quantized_parafac\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors_init(shape, rank, dtype = 'f'):\n",
    "    if dtype == 'f':\n",
    "        factors = [torch.randn((i, rank)).type(torch.float32)  for i in shape] \n",
    "    elif dtype == 'i':\n",
    "        factors = [torch.randint(-256, 256, (i, rank)).type(torch.int8) for i in shape]\n",
    "        \n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tensor in Kruskal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||factors||: 825.6332397460938, factors type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "numpy_to_struct = {np.float32 : 'f', np.int8 : 'i'}\n",
    "pytorch_to_struct = {torch.float32 : 'f', torch.int8 : 'i'}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "rank = 300\n",
    "shape = (16, 16, 9)\n",
    "\n",
    "rank_expansion = 1\n",
    "\n",
    "dtype =  torch.float32\n",
    "struct_dtype = pytorch_to_struct[dtype]\n",
    "\n",
    "factors = get_factors_init(shape, rank, dtype = struct_dtype)\n",
    "\n",
    "for i in range(len(factors)):\n",
    "    factors[i] = factors[i].repeat(1, rank_expansion)\n",
    "\n",
    "weights = torch.ones(rank * rank_expansion).type(dtype)\n",
    "    \n",
    "\n",
    "krt = KruskalTensor((weights, factors))\n",
    "t = kruskal_to_tensor(krt).to(device)\n",
    "\n",
    "tnorm = tl.norm(t.type(torch.float32))\n",
    "print('||factors||: {}, factors type: {}'.format(tnorm, t.dtype))                                                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_INIT_STARTS = 10\n",
    "\n",
    "random_init_starts = RANDOM_INIT_STARTS\n",
    "inits = ['svd'] + ['random'] * random_init_starts\n",
    "random_states = [None] + [int(torch.randint(high = 11999, size = (1,))) for _ in range(random_init_starts)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER_MAX = 50000\n",
    "\n",
    "QSCHEME = torch.per_channel_affine\n",
    "DIM = 0 if (shape[0] > rank or shape[1] > rank) else 1\n",
    "\n",
    "\n",
    "params_als_shared = {'n_iter_max' : N_ITER_MAX,\\\n",
    "                    'tol' : 1e-18,\\\n",
    "                    'normalize_factors' : True,\\\n",
    "                    'svd' : 'numpy_svd',\\\n",
    "                    'verbose' : 0}\n",
    "\n",
    "params_als = {'orthogonalise' : False,\\\n",
    "              'non_negative' : False,\\\n",
    "              'mask' : None,\\\n",
    "              'return_errors' : False,\\\n",
    "              'stop_criterion' : 'rec_error_decrease'}\n",
    "\n",
    "params_qint = {'dtype' : torch.qint8,\\\n",
    "               'qscheme' : QSCHEME,\\\n",
    "               'dim' : DIM}\n",
    "\n",
    "params_qals = {'qmodes' : [0, 1, 2],\\\n",
    "               'return_scale_zeropoint' : True,\\\n",
    "               'stop_criterion' : 'rec_error_decrease'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = rank\n",
    "\n",
    "run_id = 7\n",
    "init = inits[run_id]\n",
    "random_state = random_states[run_id]\n",
    "\n",
    "# out_als = None\n",
    "out_als = parafac(t.type(torch.float32), rank,\\\n",
    "              init=init,\\\n",
    "              random_state=random_state,\\\n",
    "              **params_als_shared,\\\n",
    "              **params_als,\\\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||tensor - als_factors||: 761.3245849609375\n"
     ]
    }
   ],
   "source": [
    "t_als = kruskal_to_tensor(out_als) \n",
    "rec_error_als = tl.norm(t - t_als)\n",
    "\n",
    "print('||tensor - als_factors||: {}'.format(rec_error_als))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 54093.6172, 158965.0312,  81790.3047,  83904.1797,  39945.5703,\n",
       "        354158.7812,  70380.8203,  69260.3438,  34719.7891,  10057.7256,\n",
       "         29761.4082, 123878.2188, 119851.8203,  56869.3164,  26036.2031,\n",
       "        282181.0312,   8585.0596,  49806.5469, 108293.4375,  50925.9727,\n",
       "        111832.8672,  14260.6191,  40978.8828, 105224.0859,  36855.5508,\n",
       "         28522.7871,  36732.5898,  22854.8379,  67397.7734,  63997.3984,\n",
       "        109245.6328,  18494.9160,  90700.0391,  91785.3750, 164986.8594,\n",
       "         66721.2422,  43028.9922,  50304.2148,  13145.3779,  15411.0166,\n",
       "        126044.5391,  98746.8359,  29375.3887,  21416.2305, 127061.3750,\n",
       "         19309.2578, 102058.7344,  42902.6758,  14679.6973,  56093.6484,\n",
       "         43329.9805,  39214.4922, 202726.3594, 131725.0312,  42330.4453,\n",
       "         66052.9688, 147129.0938,  66159.5781,  41001.3516,   6542.1289,\n",
       "         18728.8164,  37449.5273,  96383.8516, 164784.3750,  49502.8828,\n",
       "        179838.0000,  74551.7031,  86621.6094,  15266.9326, 109238.9297,\n",
       "         43420.4336, 231874.1094,  11620.3955,  74710.5156,  43542.1172,\n",
       "        209951.1875,  13781.6484,  32622.3809,   8009.7520,  30299.9590,\n",
       "         21005.9316,  47133.7422,   5012.4790, 278305.1875, 153263.7031,\n",
       "         49460.2148,  45438.6328,  69775.6016,  45560.9258,  88122.5859,\n",
       "         50469.9922,   8569.1025, 104598.7656,  65071.4688, 150032.5781,\n",
       "        172056.7969,  76660.7656, 325740.7812,  31522.2754,  16394.0293,\n",
       "         17261.6406,  14195.9971,  63264.4883,  62602.4883,  31647.0918,\n",
       "         50106.0625,  80931.8125,  68015.2734,  48599.1758,  89912.0312,\n",
       "         28940.3477,  38015.0781,  29283.6191,  72479.5234,  26505.7852,\n",
       "        201918.2812, 148408.3125,  38477.8398,  76407.9297,  76809.0703,\n",
       "        237151.0781, 107327.2578, 109260.0469,  50085.6523,  27610.0137,\n",
       "         83313.6016,  60342.4297,  28407.8184, 146758.4844,  85496.2500,\n",
       "         20709.8477,  91758.1016,  57777.6914,  18065.9707,  40765.3008,\n",
       "        139193.2812,  14872.0625,  37366.2656,  90971.0156,  28884.6387,\n",
       "        197035.5469,  19879.1035,   8476.5664, 221703.6094,  84668.6094,\n",
       "        217613.8125,  20623.5410, 103377.9844,  76122.0469,  30766.3750,\n",
       "         56116.7930,  46217.8555, 118442.6094,  60912.9961,  11379.9297,\n",
       "         49810.5938,  45501.8008,  57384.9531,  46993.1797, 139752.6875,\n",
       "         79899.2422,  70773.0078,  41129.3438, 135211.5938, 140694.4844,\n",
       "        204804.9531,  60370.0273, 148928.0000, 108101.2812,  34011.3594,\n",
       "         50978.2539, 185552.3125,  39691.0898,  43698.9727,  94663.0391,\n",
       "        149241.2500, 100441.0469,  84188.4844,  88454.0000,  77095.0078,\n",
       "         34605.5508, 168387.8281,  76295.3516,  20822.5508,  23080.1348,\n",
       "         62251.3789,  18202.7520,  14632.9336,  63117.6797,  81864.7656,\n",
       "         75863.8984,   2066.0310,  26185.2129,  46779.5898,  50257.4766,\n",
       "         36934.9727,  69419.8047,  58409.6211,  78767.9219,  33994.2656,\n",
       "         37243.6953,  53393.6211,  71644.9141, 131758.5000, 159111.2344,\n",
       "        123295.7188,  70040.8672, 151207.0312,  10665.3535,  12983.0225,\n",
       "        118897.2812,  11099.6924, 100759.5312,  21855.0625,   6657.6133,\n",
       "         87214.9375,  56566.4102, 107616.0234, 100435.0391,  29759.5137,\n",
       "         55864.3711,  65070.9297,   3888.8738, 116670.1641, 129311.3359,\n",
       "         81472.3359,  97354.4297,  33750.2773,  37179.4531,  77566.0859,\n",
       "         10974.2842,  38416.4570, 152657.2500,  63817.9727,  47713.6719,\n",
       "         57001.8125,  88501.1172, 164802.2812, 119872.7734, 110134.1641,\n",
       "         62593.7617,  66097.3750,  23794.0000, 107801.2266, 110947.7422,\n",
       "          8782.0791,  95001.2188,  12785.3848, 130308.6172,  37384.1797,\n",
       "         75802.2188,  77051.1172, 181160.9844,  10960.4043,   8090.2368,\n",
       "         33655.7578,  96378.4062,  73205.3203,  68681.2188,  66255.5234,\n",
       "         49949.2188, 103204.3672, 225402.4688, 208324.9219,  46490.8633,\n",
       "         30715.1777,  32463.4375,  11645.4668,  86704.4219, 124782.7266,\n",
       "        102600.1484,  46282.0703,  70743.2109,  41731.4102,  32964.9336,\n",
       "         65833.8594,  35833.3750,  72980.7891,  90566.3984,  57806.6562,\n",
       "        116278.6875,  90181.8125, 118292.3672,  19726.5293,  65574.9766,\n",
       "         34025.9180,  34120.0586,  17181.9375, 164842.3750,  48298.4688,\n",
       "         44264.7539,  28175.5039,  52715.2891,  90526.2500, 117216.3359,\n",
       "        206713.4062,  58212.5273, 117562.0469,  51926.2695, 116896.7266],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tl.norm(out_als.factors[0], 2, axis = 0))\n",
    "    \n",
    "out_als.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS + post quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = \\\n",
    "[(torch.per_tensor_symmetric, None),\n",
    " (torch.per_channel_symmetric, 0),\n",
    " (torch.per_channel_symmetric, 1),\n",
    " (torch.per_tensor_affine, None),\n",
    " (torch.per_channel_affine, 0),\n",
    " (torch.per_channel_affine, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.per_tensor_symmetric None\n",
      "||tensor - als_factors_quantized||: 17715.49609375\n",
      "||tensor - tensor_quantized||: 6.4526166915893555\n",
      "\n",
      "torch.per_channel_symmetric 0\n",
      "||tensor - als_factors_quantized||: 13258.7265625\n",
      "||tensor - tensor_quantized||: 5.234733581542969\n",
      "\n",
      "torch.per_channel_symmetric 1\n",
      "||tensor - als_factors_quantized||: 15332.98828125\n",
      "||tensor - tensor_quantized||: 5.4516167640686035\n",
      "\n",
      "torch.per_tensor_affine None\n",
      "||tensor - als_factors_quantized||: 17446.341796875\n",
      "||tensor - tensor_quantized||: 6.499914169311523\n",
      "\n",
      "torch.per_channel_affine 0\n",
      "||tensor - als_factors_quantized||: 12275.568359375\n",
      "||tensor - tensor_quantized||: 4.930178165435791\n",
      "\n",
      "torch.per_channel_affine 1\n",
      "||tensor - als_factors_quantized||: 10311.3203125\n",
      "||tensor - tensor_quantized||: 4.973144054412842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qscheme, dim in schemes[:]:\n",
    "    params_qint['qscheme'] = qscheme\n",
    "    params_qint['dim'] = dim\n",
    "    \n",
    "    qfactors_post = []\n",
    "    scales_post = []\n",
    "    zero_points_post = []\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    weights_post = copy.deepcopy(out_als.weights)\n",
    "    \n",
    "    for i in range(len(out_als.factors)):\n",
    "        q, s, z = quantize_qint(out_als.factors[i].cpu(),\n",
    "                                **params_qint,\n",
    "                                return_scale_zeropoint = True)\n",
    "        q = q.to(device)\n",
    "        s = s.to(device)\n",
    "        z = z.to(device)\n",
    "        \n",
    "        qfactors_post.append(q)\n",
    "        scales_post.append(s)\n",
    "        zero_points_post.append(z)\n",
    "\n",
    "        weights_post /=  tl.norm(q, order = 2, axis = 0)\n",
    "\n",
    "#     print(weights_post)\n",
    "\n",
    "\n",
    "    qt_post = kruskal_to_tensor(KruskalTensor((weights_post, qfactors_post)))\n",
    "    rec_error_qpost = tl.norm(t - qt_post)\n",
    "\n",
    "    print(qscheme, dim)\n",
    "    print('||tensor - als_factors_quantized||: {}'.format(rec_error_qpost))\n",
    "    \n",
    "    \n",
    "    qt = quantize_qint(t.cpu(),\n",
    "           **params_qint,\n",
    "           return_scale_zeropoint = False).to(device)\n",
    "    print('||tensor - tensor_quantized||: {}\\n'.format(tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([ 39,  -1,  -2, -14, -18,  40,  21,  27,  -1, -20,  15,  -3, -23,  55,\n",
       "           47,  32,  16,   0,   4, -13,   3,  12, -14, -49,  10,   2,  19, -18,\n",
       "           12, -15, -17,  39,  11,  40,  16, -45, -18, -16,  -4, -41,  33, -56,\n",
       "           43,  38, -27,   2,  30,  -5,  27,  -4,   4,  -9, -52, -11,  22, -63,\n",
       "            9, -24,  45,  23,  -2,  31,  54,  11, -10,  -3, -19, -10,  10,  23,\n",
       "          -28, -30,  54, -28, -32,  38,  -9,  -7, -39, -58,  -1, -34,  11,  18,\n",
       "           18, -67, -17, -50, -48,  44,  24, -17,  -5,  49, -35,  14, -20, -18,\n",
       "           -5, -18,  -3,  16, -31, -27,  15,   4, -12,  -4, -37, -13,  -3,  14,\n",
       "           51,  12, -22,  30,  11,  10,  -7,  18,   8, -18,  21,   4,  24,   0,\n",
       "           30, -47, -10,  29,  48, -17, -13,  59,  35,   1, -40,  -2,   2,  -9,\n",
       "          -22,  23,  59,  39,  -5,  11, -41,   1, -12, -43,  57,  25,  13,   5,\n",
       "           20,  -4, -40, -32, -34,  19,  24,   8,  -5,  15,  18, -13,  12,   5,\n",
       "           -6,  32, -15, -15, -11,  10,   3, -33,   9,  23,  -3,   3,  26,   6,\n",
       "          -34, -27, -34,  -9,  49,  20,   4,  -1,  -1, -31,   1, -13,  38, -11,\n",
       "           16, -34, -35, -13, -20, -23,  30, -31,   2,  16,   9, -53,   8, -10,\n",
       "           -4, -43,  13,  17, -13, -25,  47,  -6, -44, -34,  10, -19,  40,  17,\n",
       "            3, -11,  -5,  24,  11, -41,  21,   5,  13,  16, -20,  -7,  -2, -41,\n",
       "          -17, -16, -23,  -6, -49, -23, -25,  -1,   8,  51,   8, -24, -10,  11,\n",
       "           -4,   2, -24,  29, -35,   7,  28,  -1, -19, -21,  27, -15,  -2,  -4,\n",
       "            8,   0,  56,  20,  -9, -11, -26,  35,  35, -37, -18,  36,  44,   8,\n",
       "           10,   2, -10,   0,   7,  10,  36,  13,   0,  -8, -13, -58, -41,   3,\n",
       "          -29,  22, -28, -47,  21,  45], device='cuda:0', dtype=torch.int32),\n",
       "  tensor([-34,  33,  29, -29,   5, -40,  54, -23,  33, -34, -30,  18,  49,  36,\n",
       "          -60,  32, -48, -55, -41,  -3, -48, -56, -51,  36, -42,  46,  37, -17,\n",
       "          -46, -45,  28,  12, -47, -37, -28,  38,  39,   7, -27, -19, -37,  34,\n",
       "           38, -51,  21, -53,  51,  26,  45, -57, -38,  44,  35,  34,  58, -30,\n",
       "          -38,  12,  26,   0, -27,  28, -26,  38, -35, -38, -37,  18, -14,  71,\n",
       "          -33, -46,   8,  29,  32,  36, -44,   7, -29,  61,   7,  33,  39, -38,\n",
       "          -33,  58,  50, -45, -34,  67,   3, -59,  22,  41, -34, -40, -25, -42,\n",
       "          -36,  50,  20, -43,  24, -33, -26,  45,  28, -60, -39, -23, -31,  21,\n",
       "          -24, -40, -55, -19, -43, -50, -44,  30,  45,  24, -36,  -5,  37, -16,\n",
       "           29, -62, -51,  31,  34,  39, -38, -27, -33,  34, -44,   7,  19, -59,\n",
       "          -52, -11,   3,  30, -39, -30, -60,  43, -37,  57,  19,  32,  29,  22,\n",
       "          -26, -47,  27,  51,  58, -19, -32,  40, -47, -44,  37, -34,  33,  42,\n",
       "          -24, -26,  60, -36, -50, -32,  32,  37,  64, -43,  17, -32, -55,  42,\n",
       "           20, -35, -10,  36,  41,  40, -40,  29, -40,  17, -19,   5, -33, -52,\n",
       "           28, -48, -53, -35, -47, -39, -40,  44,  35,  30, -25,  36,  25,  22,\n",
       "           44,  24,  47, -25,  35,  31,  33,  39,  35,  35,  38,  44, -55, -44,\n",
       "          -32, -36,  29,  52,  55, -29,  33, -37, -56, -49,  62, -42,  50,  49,\n",
       "          -40, -32,  54, -22,  24,  38, -52, -15, -40, -15, -34,  42, -19,  52,\n",
       "          -45,  42,  36,  21, -39,  37, -24,  50, -11, -37,  53,  31, -23, -66,\n",
       "           36, -27,  34,  36,  36, -47,  58, -40, -47,  49, -29,  33, -54, -22,\n",
       "          -40,  52, -38,  10,  25,  39, -21,  21,  33, -37,   1, -33, -29, -42,\n",
       "          -35,  56, -69, -37,  37, -31], device='cuda:0', dtype=torch.int32),\n",
       "  tensor([ -60,   29,   -5,  -14,  -58,  -44,   50,  -95,  -38,  -67, -101,  -15,\n",
       "            69,   10,   13,  -15,  -66,  -11,   15,   44,   -1,  -33,    8,  -85,\n",
       "            25,  -58,  127,   70,  117,  -53,   30,   71, -127,   27,  -22,  -18,\n",
       "           -49,   30,  -60,  -99, -127,  -62,   44,   18,   57,  -66,  -34,  -43,\n",
       "            36,  -24,  -23,  -75,    2,  -65,  -77,  -15,   46,  -18,  -31,    7,\n",
       "           -60,  -46,   48,   46,  -50,   13,   19,  -36,  -39,    9,  -46,   76,\n",
       "           -48,  -55,  -16,   46,  -68,   72,   13,  -51,   57,    6,  -28,  -18,\n",
       "           -73,   20,  -17,   34,  -70,  -41,   -6,  -43,  -24,    9,    9,   10,\n",
       "            37,   49,  -12,   89,   23,   47,   23,    5,  -19,  127,   23,  -33,\n",
       "           -52,  -52, -114,   70,   68,   -6,  -91,   38,  -33,   -3,   33,  -39,\n",
       "           -41,  -44,   52,   67,  -53,  -15,  -56,  -30,   84,   25,   56,  102,\n",
       "            14,    4,   -2,   76,   82,   34,   42,   29,   57,   13,   69,  -26,\n",
       "           -45,   14,    8,   26,  -37,   16,   34,  -58,   17,  -40,  -36, -101,\n",
       "            -7,   33,   64,   -7,  -28,  -34,  -69,    6,   16,   20,  -45,   19,\n",
       "           -31,   25,   48,  -33,   33,   78,   35,   44,   21,   69,   66,  -49,\n",
       "            84,    0,  -70,   36,   57,  -75,  -30,  -27,   -2,  -21,   47,   61,\n",
       "           -95,   74,   57,   -5, -101,    2,  -83,  -27,  -39,  -34,    5,  -85,\n",
       "            47,   -5,   20,   69,  -19,   20,   63,  -69,  -29,   -8,   39,  -20,\n",
       "            -7,   13,  -18,  -20,  -19,  -35,   42, -109,   98,  -37,   87,  -40,\n",
       "           -13,   15,  -21,  -84,   98,    1,  -59,   12,   47,   63, -128,   61,\n",
       "           -67,   40,   23,  -41,  119,  100,   46,  -19,  -80,    9,  -32,   60,\n",
       "           -17,  -35,   48,  -22,   31,   42,   51,  -71,   -1,    5,   83,  -39,\n",
       "           -27,  -37,   70,   19,   -1,  -36,    4,  -30,   19,   80,   14,  -14,\n",
       "            -6,   42,   75,  -13,   34,  -48,  -93,   38,   -2,   19,   33,   28,\n",
       "           -45,  -19,  -15, -109, -128,  -18,  -71,  -49,  108,   61,   23,   25],\n",
       "         device='cuda:0', dtype=torch.int32)],\n",
       " [tensor([0.0032, 0.0035, 0.0037, 0.0038, 0.0032, 0.0037, 0.0034, 0.0038, 0.0036,\n",
       "          0.0034, 0.0036, 0.0042, 0.0038, 0.0032, 0.0033, 0.0043, 0.0035, 0.0034,\n",
       "          0.0034, 0.0039, 0.0034, 0.0041, 0.0035, 0.0034, 0.0039, 0.0037, 0.0032,\n",
       "          0.0033, 0.0037, 0.0039, 0.0037, 0.0035, 0.0040, 0.0036, 0.0038, 0.0043,\n",
       "          0.0039, 0.0033, 0.0041, 0.0040, 0.0032, 0.0038, 0.0038, 0.0036, 0.0036,\n",
       "          0.0040, 0.0037, 0.0036, 0.0031, 0.0039, 0.0040, 0.0037, 0.0036, 0.0033,\n",
       "          0.0037, 0.0032, 0.0039, 0.0039, 0.0034, 0.0036, 0.0042, 0.0034, 0.0038,\n",
       "          0.0035, 0.0036, 0.0035, 0.0035, 0.0038, 0.0042, 0.0037, 0.0029, 0.0036,\n",
       "          0.0035, 0.0037, 0.0039, 0.0039, 0.0041, 0.0038, 0.0039, 0.0038, 0.0038,\n",
       "          0.0029, 0.0039, 0.0040, 0.0039, 0.0035, 0.0035, 0.0039, 0.0032, 0.0037,\n",
       "          0.0037, 0.0047, 0.0030, 0.0038, 0.0040, 0.0041, 0.0039, 0.0037, 0.0036,\n",
       "          0.0036, 0.0035, 0.0041, 0.0041, 0.0039, 0.0035, 0.0040, 0.0037, 0.0037,\n",
       "          0.0037, 0.0038, 0.0038, 0.0040, 0.0041, 0.0034, 0.0038, 0.0038, 0.0036,\n",
       "          0.0040, 0.0035, 0.0037, 0.0038, 0.0038, 0.0035, 0.0044, 0.0034, 0.0039,\n",
       "          0.0031, 0.0040, 0.0037, 0.0034, 0.0041, 0.0042, 0.0034, 0.0039, 0.0035,\n",
       "          0.0036, 0.0036, 0.0044, 0.0039, 0.0034, 0.0038, 0.0034, 0.0033, 0.0035,\n",
       "          0.0036, 0.0038, 0.0039, 0.0030, 0.0031, 0.0040, 0.0040, 0.0034, 0.0040,\n",
       "          0.0033, 0.0038, 0.0037, 0.0043, 0.0036, 0.0032, 0.0041, 0.0033, 0.0033,\n",
       "          0.0035, 0.0040, 0.0043, 0.0038, 0.0037, 0.0036, 0.0035, 0.0040, 0.0035,\n",
       "          0.0034, 0.0041, 0.0036, 0.0034, 0.0037, 0.0037, 0.0036, 0.0038, 0.0044,\n",
       "          0.0040, 0.0036, 0.0038, 0.0039, 0.0039, 0.0030, 0.0037, 0.0039, 0.0028,\n",
       "          0.0039, 0.0032, 0.0037, 0.0037, 0.0040, 0.0037, 0.0032, 0.0040, 0.0035,\n",
       "          0.0036, 0.0038, 0.0034, 0.0030, 0.0037, 0.0037, 0.0037, 0.0036, 0.0037,\n",
       "          0.0033, 0.0032, 0.0036, 0.0037, 0.0033, 0.0032, 0.0036, 0.0041, 0.0039,\n",
       "          0.0033, 0.0035, 0.0034, 0.0038, 0.0038, 0.0037, 0.0037, 0.0035, 0.0041,\n",
       "          0.0037, 0.0037, 0.0034, 0.0035, 0.0035, 0.0039, 0.0034, 0.0035, 0.0037,\n",
       "          0.0036, 0.0034, 0.0037, 0.0039, 0.0039, 0.0038, 0.0034, 0.0036, 0.0039,\n",
       "          0.0036, 0.0037, 0.0035, 0.0038, 0.0037, 0.0036, 0.0037, 0.0037, 0.0038,\n",
       "          0.0037, 0.0031, 0.0034, 0.0042, 0.0034, 0.0039, 0.0039, 0.0037, 0.0036,\n",
       "          0.0038, 0.0036, 0.0039, 0.0040, 0.0038, 0.0032, 0.0035, 0.0040, 0.0044,\n",
       "          0.0036, 0.0041, 0.0037, 0.0033, 0.0035, 0.0041, 0.0035, 0.0037, 0.0037,\n",
       "          0.0038, 0.0036, 0.0040, 0.0039, 0.0039, 0.0038, 0.0037, 0.0039, 0.0039,\n",
       "          0.0030, 0.0042, 0.0039, 0.0039, 0.0037, 0.0037, 0.0042, 0.0035, 0.0036,\n",
       "          0.0036, 0.0033, 0.0035], device='cuda:0'),\n",
       "  tensor([0.0040, 0.0037, 0.0039, 0.0039, 0.0041, 0.0039, 0.0036, 0.0039, 0.0035,\n",
       "          0.0036, 0.0036, 0.0041, 0.0036, 0.0039, 0.0034, 0.0040, 0.0037, 0.0036,\n",
       "          0.0038, 0.0040, 0.0037, 0.0034, 0.0037, 0.0039, 0.0038, 0.0035, 0.0039,\n",
       "          0.0044, 0.0037, 0.0036, 0.0040, 0.0041, 0.0038, 0.0039, 0.0041, 0.0039,\n",
       "          0.0039, 0.0039, 0.0033, 0.0041, 0.0040, 0.0039, 0.0040, 0.0035, 0.0041,\n",
       "          0.0036, 0.0036, 0.0041, 0.0038, 0.0035, 0.0040, 0.0035, 0.0040, 0.0039,\n",
       "          0.0033, 0.0041, 0.0039, 0.0041, 0.0040, 0.0036, 0.0037, 0.0039, 0.0040,\n",
       "          0.0038, 0.0042, 0.0038, 0.0041, 0.0041, 0.0043, 0.0035, 0.0039, 0.0038,\n",
       "          0.0038, 0.0038, 0.0038, 0.0039, 0.0038, 0.0035, 0.0040, 0.0034, 0.0037,\n",
       "          0.0040, 0.0036, 0.0039, 0.0040, 0.0033, 0.0036, 0.0039, 0.0041, 0.0032,\n",
       "          0.0045, 0.0034, 0.0042, 0.0038, 0.0041, 0.0040, 0.0039, 0.0038, 0.0041,\n",
       "          0.0039, 0.0035, 0.0033, 0.0041, 0.0044, 0.0041, 0.0035, 0.0041, 0.0035,\n",
       "          0.0040, 0.0039, 0.0037, 0.0042, 0.0036, 0.0039, 0.0035, 0.0044, 0.0038,\n",
       "          0.0035, 0.0038, 0.0039, 0.0038, 0.0041, 0.0038, 0.0033, 0.0037, 0.0043,\n",
       "          0.0040, 0.0036, 0.0036, 0.0040, 0.0033, 0.0039, 0.0038, 0.0041, 0.0040,\n",
       "          0.0039, 0.0036, 0.0041, 0.0042, 0.0035, 0.0037, 0.0039, 0.0036, 0.0040,\n",
       "          0.0038, 0.0040, 0.0033, 0.0038, 0.0037, 0.0034, 0.0041, 0.0039, 0.0040,\n",
       "          0.0041, 0.0040, 0.0037, 0.0039, 0.0039, 0.0034, 0.0041, 0.0040, 0.0039,\n",
       "          0.0040, 0.0039, 0.0039, 0.0038, 0.0036, 0.0038, 0.0039, 0.0043, 0.0034,\n",
       "          0.0040, 0.0037, 0.0040, 0.0040, 0.0039, 0.0033, 0.0037, 0.0043, 0.0041,\n",
       "          0.0034, 0.0039, 0.0040, 0.0043, 0.0045, 0.0040, 0.0041, 0.0036, 0.0039,\n",
       "          0.0042, 0.0039, 0.0034, 0.0042, 0.0032, 0.0040, 0.0038, 0.0039, 0.0037,\n",
       "          0.0036, 0.0038, 0.0037, 0.0040, 0.0038, 0.0038, 0.0040, 0.0040, 0.0042,\n",
       "          0.0039, 0.0033, 0.0030, 0.0037, 0.0030, 0.0037, 0.0041, 0.0040, 0.0040,\n",
       "          0.0040, 0.0038, 0.0041, 0.0039, 0.0039, 0.0036, 0.0037, 0.0038, 0.0040,\n",
       "          0.0041, 0.0040, 0.0035, 0.0035, 0.0039, 0.0035, 0.0039, 0.0036, 0.0037,\n",
       "          0.0035, 0.0039, 0.0035, 0.0037, 0.0039, 0.0036, 0.0036, 0.0041, 0.0040,\n",
       "          0.0038, 0.0037, 0.0039, 0.0039, 0.0041, 0.0040, 0.0039, 0.0042, 0.0037,\n",
       "          0.0035, 0.0040, 0.0040, 0.0033, 0.0039, 0.0037, 0.0040, 0.0034, 0.0046,\n",
       "          0.0039, 0.0034, 0.0039, 0.0039, 0.0033, 0.0041, 0.0033, 0.0039, 0.0040,\n",
       "          0.0040, 0.0038, 0.0033, 0.0037, 0.0040, 0.0037, 0.0041, 0.0040, 0.0036,\n",
       "          0.0041, 0.0039, 0.0035, 0.0038, 0.0032, 0.0039, 0.0039, 0.0040, 0.0038,\n",
       "          0.0039, 0.0039, 0.0042, 0.0038, 0.0040, 0.0038, 0.0040, 0.0035, 0.0034,\n",
       "          0.0039, 0.0040, 0.0040], device='cuda:0'),\n",
       "  tensor([0.0034, 0.0046, 0.0049, 0.0046, 0.0042, 0.0035, 0.0035, 0.0027, 0.0038,\n",
       "          0.0037, 0.0036, 0.0037, 0.0041, 0.0042, 0.0044, 0.0038, 0.0031, 0.0039,\n",
       "          0.0040, 0.0047, 0.0039, 0.0041, 0.0045, 0.0039, 0.0034, 0.0035, 0.0034,\n",
       "          0.0037, 0.0029, 0.0044, 0.0045, 0.0037, 0.0027, 0.0034, 0.0039, 0.0041,\n",
       "          0.0040, 0.0039, 0.0043, 0.0034, 0.0028, 0.0036, 0.0043, 0.0039, 0.0034,\n",
       "          0.0035, 0.0043, 0.0032, 0.0042, 0.0038, 0.0045, 0.0040, 0.0049, 0.0036,\n",
       "          0.0040, 0.0046, 0.0037, 0.0048, 0.0042, 0.0038, 0.0032, 0.0039, 0.0043,\n",
       "          0.0034, 0.0030, 0.0050, 0.0047, 0.0040, 0.0038, 0.0038, 0.0037, 0.0037,\n",
       "          0.0044, 0.0037, 0.0035, 0.0034, 0.0032, 0.0042, 0.0043, 0.0037, 0.0038,\n",
       "          0.0037, 0.0036, 0.0039, 0.0032, 0.0039, 0.0043, 0.0041, 0.0040, 0.0037,\n",
       "          0.0038, 0.0041, 0.0047, 0.0039, 0.0034, 0.0048, 0.0043, 0.0032, 0.0047,\n",
       "          0.0032, 0.0049, 0.0045, 0.0037, 0.0047, 0.0045, 0.0026, 0.0039, 0.0043,\n",
       "          0.0043, 0.0036, 0.0035, 0.0041, 0.0038, 0.0038, 0.0033, 0.0045, 0.0039,\n",
       "          0.0042, 0.0039, 0.0043, 0.0036, 0.0034, 0.0031, 0.0033, 0.0038, 0.0048,\n",
       "          0.0036, 0.0042, 0.0025, 0.0046, 0.0043, 0.0028, 0.0048, 0.0039, 0.0042,\n",
       "          0.0043, 0.0032, 0.0037, 0.0039, 0.0036, 0.0034, 0.0044, 0.0040, 0.0042,\n",
       "          0.0038, 0.0044, 0.0052, 0.0041, 0.0042, 0.0038, 0.0039, 0.0043, 0.0031,\n",
       "          0.0041, 0.0034, 0.0026, 0.0045, 0.0040, 0.0040, 0.0034, 0.0039, 0.0040,\n",
       "          0.0040, 0.0043, 0.0040, 0.0048, 0.0038, 0.0042, 0.0040, 0.0038, 0.0037,\n",
       "          0.0046, 0.0033, 0.0029, 0.0039, 0.0033, 0.0035, 0.0039, 0.0039, 0.0033,\n",
       "          0.0043, 0.0046, 0.0042, 0.0042, 0.0039, 0.0034, 0.0037, 0.0041, 0.0038,\n",
       "          0.0040, 0.0032, 0.0040, 0.0030, 0.0033, 0.0044, 0.0036, 0.0038, 0.0040,\n",
       "          0.0036, 0.0042, 0.0034, 0.0042, 0.0041, 0.0038, 0.0038, 0.0047, 0.0039,\n",
       "          0.0038, 0.0044, 0.0031, 0.0033, 0.0040, 0.0039, 0.0042, 0.0040, 0.0043,\n",
       "          0.0042, 0.0046, 0.0042, 0.0042, 0.0038, 0.0036, 0.0036, 0.0029, 0.0033,\n",
       "          0.0038, 0.0034, 0.0038, 0.0037, 0.0049, 0.0041, 0.0037, 0.0027, 0.0046,\n",
       "          0.0042, 0.0033, 0.0033, 0.0034, 0.0029, 0.0042, 0.0036, 0.0038, 0.0043,\n",
       "          0.0044, 0.0030, 0.0034, 0.0038, 0.0046, 0.0040, 0.0036, 0.0044, 0.0039,\n",
       "          0.0044, 0.0041, 0.0037, 0.0041, 0.0045, 0.0045, 0.0032, 0.0038, 0.0044,\n",
       "          0.0046, 0.0039, 0.0043, 0.0042, 0.0049, 0.0043, 0.0046, 0.0043, 0.0043,\n",
       "          0.0041, 0.0048, 0.0038, 0.0042, 0.0041, 0.0044, 0.0041, 0.0038, 0.0039,\n",
       "          0.0032, 0.0033, 0.0033, 0.0027, 0.0045, 0.0040, 0.0047, 0.0040, 0.0041,\n",
       "          0.0037, 0.0039, 0.0035, 0.0036, 0.0026, 0.0037, 0.0033, 0.0035, 0.0035,\n",
       "          0.0039, 0.0046, 0.0043], device='cuda:0')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_points_post, scales_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||tensor - qals_factors||: 10806.677734375\n"
     ]
    }
   ],
   "source": [
    "params_qint['qscheme'] = QSCHEME\n",
    "params_qint['dim'] = DIM\n",
    "    \n",
    "qout, qerrors, scales, zero_points = quantized_parafac(t, rank,\\\n",
    "                                                         init=init,\\\n",
    "                                                         random_state=random_state,\\\n",
    "                                                         **params_als_shared,\\\n",
    "                                                         **params_qals,\\\n",
    "                                                         **params_qint)\n",
    "t_approx_qals = kruskal_to_tensor(KruskalTensor(qout))\n",
    "rec_error_qals = tl.norm(t - t_approx_qals)\n",
    "\n",
    "print('||tensor - qals_factors||: {}'.format(rec_error_qals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff0b685cc18>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeRUlEQVR4nO3deXhV9b3v8fc32RkICVMSFAkIilUGLWJAcGKwnlpPq+eop4fOVls6eDvZ1vm5fc65darW6qm2liO11vagXutprVe99RYQRQUCOALiiARRAghJCJl/94+1AyEmZGdPv732/ryeJ0/2sNjrs7LJJ7/922uvZc45REQkfPJ8BxARkfiowEVEQkoFLiISUipwEZGQUoGLiIRUJJ0rq6iocOPGjUvnKkVEQm/NmjU7nHOVPW9Pa4GPGzeOmpqadK5SRCT0zGxzb7drCkVEJKRU4CIiIaUCFxEJqbTOgfemra2N2tpampubfUfpU3FxMVVVVRQUFPiOIiKyn/cCr62tpaysjHHjxmFmvuN8hHOOnTt3Ultby/jx433HERHZz/sUSnNzM+Xl5RlZ3gBmRnl5eUa/QhCR3OS9wIGMLe8umZ5PRHJTRhS4iEjWatsHj18Bu99N+kOrwKOeeOIJjj32WCZMmMCNN97oO46IZIvVi2DlXbB7S9IfWgUOdHR0cOmll/L444+zfv16Fi9ezPr1633HEpGwa2mEZ34BR82Bcacm/eFV4MCqVauYMGECRx11FIWFhcyfP5+//OUvvmOJSNit+g007YC516bk4b3vRtjdv/31Vda/V5/Ux5x0xBB+8pnJh1xm69atjBkzZv/1qqoqVq5cmdQcIpJjmvfAiv+AYz4JY6anZBUagYuIpMJzv4Lm3TD36pStot8RuJn9Fvg0sN05NyV6283AZ4BW4E3gq8653YmG6W+knCqjR49my5YDbzDU1tYyevRoL1lEJAs07YLnfwUTPwNHTE3ZamIZgf8OOLvHbU8CU5xzJwCbgKuSnCutpk+fzuuvv87bb79Na2sr999/P+eee67vWCISVs/+EloaYE7qRt8QwwjcObfczMb1uO1v3a4+D1yY3FjpFYlEuOOOO/jkJz9JR0cHF198MZMn+3k1ICIh11gX7DY45Xw4bFJKV5WMNzEvBh7o604zWwAsABg7dmwSVpca55xzDuecc47vGCISditug/ZmmJP6iYmE3sQ0s2uAduCPfS3jnFvonKt2zlVXVn7kjEAiItmjfhusvhtOmA8Vx6R8dXGPwM3sIoI3N890zrmkJRIRCaunfw6d7TD78rSsLq4CN7OzgcuB2c65puRGEhEJod3vwprfwYlfhBHpOfR0v1MoZrYYeA441sxqzewS4A6gDHjSzF4ws7tSnFNEJLMtvxnM4Iwfp22VseyF8rlebl6UgiwiIuG0801Y90eY/jUYWpW21eqTmCIiiXrqZ5BfAKdfltbVqsCBiy++mJEjRzJlyhTfUUQkbOo2wcsPwoyvQ9nhaV21Chy46KKLeOKJJ3zHEJEwWnYDRAbBqd9P+6pV4MAZZ5zBiBEjfMcQkbB5/xV49WGY+U0YXJH21WfU4WR5/Ep4/+XkPubhx8OndIYdEUmBZTdA0VA45TteVq8RuIhIPLauhY2PwqxLYdBwLxEyawSukbKIhMXS64PinvktbxE0AhcRGah3V8IbT8Kp34PiId5iqMCBz33uc8yaNYvXXnuNqqoqFi3S55RE5BCW/hQGV8KMBV5jZNYUiieLFy/2HUFEwuLt5cHXJ2+AwsFeo2gELiISK+dgyXVQNgqqL/adRgUuIhKzN/8OW56H038IBcW+06jARURi0jX6HjoWpn3ZdxpABS4iEpvXHof31sLsH0OkyHcaQAUuItK/zs5gv+8RR8HHezvCth8qcBGR/mx4BD54GWZfGRw2NkOowIEtW7Ywd+5cJk2axOTJk7n99tt9RxKRTNHZEYy+K46F4y/0neYg2g8ciEQi/PznP2fatGk0NDRw0kkncdZZZzFp0iTf0UTEt1f+BDteg3/5HeTl+05zEI3AgVGjRjFt2jQAysrKmDhxIlu3bvWcSkS862gPjjh42BSYeJ7vNB+RUSPwm1bdxMZdG5P6mMeNOI4rZlwR8/LvvPMO69at4+STT05qDhEJoRcXw663YP5iyMu88W7mJfKosbGRCy64gNtuu40hQ/wdoEZEMkB7a3CuyyOmwbGf8p2mVxk1Ah/ISDnZ2trauOCCC/jCF77A+eef7y2HiGSIdb+HPe/Cp38BZr7T9EojcMA5xyWXXMLEiRO57LL0nlVaRDJQ2z5YfguMmQkTzvSdpk/9FriZ/dbMtpvZK91uG2FmT5rZ69Hvfk5HkSQrVqzgvvvuY8mSJUydOpWpU6fy2GOP+Y4lIr7U3AMN22DeNRk7+obYplB+B9wB/L7bbVcCf3fO3WhmV0av+5v/SNBpp52Gc853DBHJBK174ZlbYfwZwVcG63cE7pxbDuzqcfN5wL3Ry/cC/5TkXCIifqz6T9hbB3Ov9Z2kX/HOgR/mnNsWvfw+cFhfC5rZAjOrMbOaurq6OFcnIpIGzfWw4jaY8AkYm/m7Eif8JqYL5h76nH9wzi10zlU756orKyv7WibRGCmV6flEJElW3gX7PoS51/hOEpN4C/wDMxsFEP2+Pd4AxcXF7Ny5M2NL0jnHzp07KS72f/B2EUmhfR/Cs3fAsf8Io6f5ThOTePcDfwT4CnBj9Ptf4g1QVVVFbW0tmTy9UlxcTFVVle8YIpJKz94BLXtg7tW+k8Ss3wI3s8XAHKDCzGqBnxAU94NmdgmwGfhsvAEKCgoYP358vP9cRCRxe3cE0yeT/xkOn+I7Tcz6LXDnXF9HL8/cvdtFRAZixW3Q1gRzrvKdZED0SUwRyW0N78Oqu+H4z0Llsb7TDIgKXERy29O3QkcrzL7cd5IBU4GLSO7aUwtr7oGpn4fyo32nGTAVuIjkruW3gHOhHH2DClxEctWut2HdfXDSV2DYWN9p4qICF5HctPxmyIvA6T/ynSRuKnARyT07Xg9Ol1Z9CQwZ5TtN3FTgIpJ7lt0IkWI47Qe+kyREBS4iueWD9fDKn+Dkb0Bp7wfYCwsVuIjklmXXQ2EpnPJd30kSpgIXkdyx7UXY8FeYdSmUjPCdJmEqcBHJHUuvh+JhMOvbvpMkhQpcRHLDltWw6Qk45TtQPNR3mqRQgYtIblh6HZSUw8nf9J0kaVTgIpL93lkBby0NdhssKvWdJmlU4CKS3ZwLRt+lhwUf3MkiKnARyW5vLYPNK4KPzBeW+E6TVCpwEclezsGSn8KQquCgVVlGBS4i2ev1v8HWGpj9Y4gU+U6TdCpwEclOXaPv4eNg6hd8p0kJFbiIZKcNf4X3X4LZV0B+ge80KaECF5Hs09kRfOqy/JjgZMVZKuI7gIhI0r3631C3AS5YBPnZW3MJjcDN7Adm9qqZvWJmi82sOFnBRETi0tEOy26AkZNg8vm+06RU3AVuZqOB7wLVzrkpQD4wP1nBRETi8tIDsPMNmHs15GX3LHGiWxcBBplZBCgB3ks8kohInDra4KmbYNTH4bhP+06TcnEXuHNuK3AL8C6wDdjjnPtbz+XMbIGZ1ZhZTV1dXfxJRUT6s+4PsHszzL0WzHynSblEplCGA+cB44EjgMFm9sWeyznnFjrnqp1z1ZWV4T59kYhksLbm4EzzVdPhmLN8p0mLRKZQPgG87Zyrc861AQ8DpyQnlojIAK29F+q3wrzcGH1DYgX+LjDTzErMzIAzgQ3JiSUiMgCtTbD8FjjyNBg/23eatElkDnwl8BCwFng5+lgLk5RLRCR2q++Gvdth3jU5M/qGBD/I45z7CfCTJGURERm4lgZYcRscPQ+OzK1Z3OzeSVJEst/Ku6BpZ7DnSY5RgYtIeO3bDc/+Ej72Kag6yXeatFOBi0h4PXcnNO8JPnWZg1TgIhJOTbvg+V/DxHNh1Am+03ihAheRcFpxO7Q25uzoG1TgIhJGjdth1UI4/kIYOdF3Gm9U4CISPs/8AtqbYfaVvpN4pQIXkXCpfw9WL4KPfx4qJvhO45UKXETCZfkt4DqCM83nOBW4iITHh5th7e9h2peDs83nOBW4iITH8p+B5cHpP/KdJCOowEUkHHa+CS8shuqvwtDRvtNkBBW4iITDUzdBfiGcdpnvJBlDBS4imW/7RnjpQZjxdSg7zHeajKECF5HMt+wGKBwMp37fd5KMogIXkcy27SVY/2eY+S0YXO47TUZRgYtIZlt2AxQNhVmX+k6ScVTgIpK5tq6B1x6DU74Dg4b7TpNxVOAikrmWXAeDRsDMb/pOkpFU4CKSmTY/B2/+HU77PhSV+U6TkVTgIpKZll4Hg0fC9K/7TpKxVOAiknneegreeRpOvwwKS3ynyVgqcBHJLM4Fo++yI+Ckr/pOk9ESKnAzG2ZmD5nZRjPbYGazkhVMRHLUG/8PtqyEM34EBcW+02S0SIL//nbgCefchWZWCOi1jojEzzlY8lMYNhZO/JLvNBkv7gI3s6HAGcBFAM65VqA1ObFEJCdt/D+w7QU4706IFPpOk/ESmUIZD9QB95jZOjO728wG91zIzBaYWY2Z1dTV1SWwOhHJap2dsPR6GHE0nDDfd5pQSKTAI8A04NfOuROBvcBHzjDqnFvonKt2zlVXVlYmsDoRyWrr/wzbX4U5V0F+orO7uSGRAq8Fap1zK6PXHyIodBGRgensCI55UnkcTDnfd5rQiLvAnXPvA1vM7NjoTWcC65OSSkRyy8v/G3ZsCkbfefm+04RGoq9TvgP8MboHyluAdtoUkYHpaAtG34cfDxPP9Z0mVBIqcOfcC0B1krKISC564b/gw3fgc/dDnj5bOBD6aYmIP+0tsPxmGH0SfOxs32lCRwUuIv6s/T3s2QJzrwEz32lCRwUuIn607YPlt8DYWXD0PN9pQkk7W4qIH6sXQeP7cOEijb7jpBG4iKRfSyM88wsYPxvGneY7TWipwEUk/VYthKYdMO9a30lCTQUuIunVvAdW3A7H/AOMmeE7TaipwEUkvZ7/NTTvhrlX+04SeipwEUmfpl3w3J1w3KfhiBN9pwk9FbiIpM+zv4SWBo2+k0QFLiLp0VgHK38THG3wsMm+02QFFbiIpMeK26B9X3DEQUkKFbiIpF79Nlh9N5zwr1BxjO80WUMFLiKp98yt0NkOsy/3nSSrqMBFJLV2b4E1v4OpX4ARR/lOk1VU4CKSWstvDr6f8WO/ObKQClxEUmfXW7DuD3DSRTBsjO80WUcFLiKp89TPIL8ATv+h7yRZSQUuIqlRtwleegCmfw3KDvedJiupwEUkNZbdAJFBcNoPfCfJWipwEUm+91+BVx+Gk78Bgyt8p8laKnARSb5lN0DREDjlO76TZDUVuIgk13vrYOOjMOtSKBnhO01WS7jAzSzfzNaZ2aPJCCQiIbf0ehg0HGZ+y3eSrJeMEfj3gA1JeBwRCbstq+D1v8Ep34Xiob7TZL2ECtzMqoB/BO5OThwRCbUlP4WSCpixwHeSnJDoCPw24HKgs68FzGyBmdWYWU1dXV2CqxORjPX20/D2U3D6ZVBU6jtNToi7wM3s08B259yaQy3nnFvonKt2zlVXVlbGuzoRyWTOwdLroGwUVF/sO03OSGQEfipwrpm9A9wPzDOzPyQllYiEy5tL4N3ngo/MFwzynSZnxF3gzrmrnHNVzrlxwHxgiXPui0lLJiLh4Fww9z10DEz7su80OUX7gYtIYjY9Ae+tDQ4XGynynSanRJLxIM65ZcCyZDyWiIRIZycsuQ6Gj4epn/edJudoBC4i8dvwCHzwMsy5MjhsrKSVClxE4tPZERzzpOJjcPy/+E6Tk5IyhSIiOeiVP0HdRrjwHsjL950mJ2kELiID19EOy26Ew6bApH/ynSZnaQQuIgP30v2w602Y/1+Qp3GgL/rJi8jAtLfCspvgiBPh2HN8p8lpKnARGZh198Ged2HuNWDmO01OU4GLSOzammH5LTDmZJjwCd9pcp7mwEUkdmvugYb34J/v0ug7A2gELiKxad0LT98K406Ho2b7TiNoBC4isVr1n7B3O/zrfb6TSJRG4CLSv+Z6WHE7HH0mjJ3pO41EqcBFpH8r74J9u2DeNb6TSDcqcBE5tH0fwrN3BPt8jz7JdxrpRgUuIof23J3QsgfmXu07ifSgAheRvu3dCc//OjjeyeHH+04jPajARaRvK24Ldh+cc5XvJNILFbiI9K7hg2DXwRM+CyOP851GeqECF5HePXMrdLTC7Ct8J5E+qMBF5KP21ELNb4PzXJYf7TuN9EEFLiIftfwWcA5mX+47iRyCClxEDvbhO8EhY6d9GYaN9Z1GDkEFLiIHe+pnYPlwxo98J5F+xF3gZjbGzJaa2Xoze9XMvpfMYCLiwY434MXFMP0SGHKE7zTSj0SORtgO/NA5t9bMyoA1Zvakc259krKJSLo9dSNEiuG0H/hOIjGIewTunNvmnFsbvdwAbABGJyuYiKTZB+vh5YdgxgIoHek7jcQgKXPgZjYOOBFY2ct9C8ysxsxq6urqkrE6EUmFZTdAYSmcqtnQsEi4wM2sFPgT8H3nXH3P+51zC51z1c656srKykRXJyKpsO1F2PAIzPo2lIzwnUZilFCBm1kBQXn/0Tn3cHIiiUjaLb0eiofCzG/7TiIDEPebmGZmwCJgg3Pu1uRF+qibVt3Exl0bU7kKidNxI47jihn6qHWo1dbApidg3rUwaJjvNDIAiYzATwW+BMwzsxeiX+ckKZeIpMuSn0JJOZz8Td9JZIDiHoE7554BLIlZ+qQRnkiKbH4W3loKZ/0vKCrznUYGSJ/EFMlVzgWj79LDYPrXfKeROKjARXLVW8tg8wo4/YdQWOI7jcRBBS6Si5yDpdfBkNEw7Su+00icVOAiuej1v0Htajjjx1BQ7DuNxEkFLpJrukbfw46EE7/oO40kQAUukms2Php88nLOlZBf4DuNJEAFLpJLOjuDT12WT4DjP+s7jSQokcPJikjYvPowbF8PFyyCfP36h51G4CK5oqM9OOLgyEkw+XzfaSQJwvEneNdb0FwPgyugpELvmovE4+UHYecb8Nn7IE9jt2wQjgJ/7k5YffeB64VlMLg8KPPBFQeKfXAFDK6MXu52f8Egf9mzgHOOfW0dNDa3U9/cTmNLO43N7TQ0tzFl9FDGjNCHQDJeRxssuxEOPwEmfsZ3GkmScBT4jG/AUXOhaQfsrYO9O6OXd8CercE76nt3QGdb7/++sDQ4WE/Pgt9/uSJ6f2XWFX5LewcNzUHhNra0U9/cFi3f4HpDcxsN+wu52237LwffOzpdr4//swtPUIGHwbo/wO7N8PkHwdJyCCNJg3AUeOXHgq9DcQ6a90DTzqDM95f9jgO37a2D+q2w7aXgcl+FXzD4owXffZTf87YUfAy5vaNzf4F2L9aghA+MgLtGw8HI+EDxdhVya0dnv+sqjOQxpDhCaVGEsuICSosijBlRQllxhLKu2/bf3/UVLDd6ePb8sctabc2w/GYYXQ3H/IPvNJJE4SjwWJgFxzIeNAzKj+5/eeegpb5bwdd1K/7o9aYd0LAN3n85uNzR2vtjdRV+SQWupIK2QeW0Fg6nqWA4TZFh1OcPY7cNZTdD2OHK2N1WQEPX6La5nYaWA6PirtHwvraOfjchP8/2F2ppUQFlxREOH1JMaY/bynqUc/cSHlyUT1Ekf4A/bAmVtfcGA5fz7tToO8tkT4EPlFlwBpLioVB+9P553q4Rb/fRbdf1lr27cY07sKYd5O3bSaRlF0UtuxjU9iGD63dT9uFuhrpNlFs95TQw0nof4Te5Ij60IdTbEOrzg5LfVzCc1uLhtA0rp7MkmNLJK60kv7SSktIhB5VvaXGEsqICigvyMP1CyqG0NsHTP4cjT4Wj5vhOI0mWFQXe3NZxYL42OqJt6DHN0H10e9BtXVMOh5jn7a6kMJ+y4pGUFh1BWXEBZSMOTC10H/GWFeUzNL+V4exhqNvDkI49DG7fTXHbLgY176KkaSej90/zbIyO9pt7X2lkUC/TON3m7Euic/tdb9wWDtZISwI1i6DxA7jwHv2fyEKhKPD/XlfL8k07DhoZd5/rjWWetyiSd9DcbVlxhLEjSigtjjCkx+i2tCh6Wy/TD/l5KfolcA5a90anbrrN2Xe9Wbt/mqcO6jYG39v7Kvzi6Px9ee9z9vuLP3p/Yal+ubNRSwM884tgB4Bxp/pOIykQigJ/c/teajbvoqwoKNVRQ4s5ZmR0KiFarEP2l2/B/tId0u3Nt8JIhu/3agZFpcHXiPH9L99V+D3n7PfP43e7XLcpWvj7en+sSHH/e+Z0v1+FHw4rfxP84Z93re8kkiLmXP/TBslSXV3tampq0rY+6aF1b+8F33PXzK7b25p6f5z8ogMFXzw0mLIpLA2+F5Ud4nr0D1T36/o4d2rs2w23nwBjZ8HnH/CdRhJkZmucc9U9b9dvTy4pHBx8DT8ytuVbm/oo+G7TPK2NUP9e8MehtfHA91hFig9d8H1ej/5h6Hm9YJBeHQA8/6tgt9q5V/tOIimkApe+FZZA4VgYNnZg/66zMxi9dxV6S0O3gm+ElsZDX2+uh/ptB9/f1z77PVnegbLfX/AJXA/jq4SmXfDcr4JPXI76uO80kkIh+58poZCXd2A+P1naW3sp/OgfhpbGA/f1db1+68HX2/bGvu5IcYxTRDFMGRWVBo+XylcJK24PtnWORt/ZTgUu4RAphMgIKBmRnMfr7AxKvNfC7/6KoY/rzXuifxS67m+EzvbY1m35PUb8CUwZFZUGHyTrepXQuB1WLYQpF8Bhk5Lzs5KMlVCBm9nZwO1APnC3c+7GpKQSSbW8vGC0XFQGZUl6zPaWAU4ZdX8FsRfqaw++PqBXCYOCQodg99I5VyVpoySTxV3gZpYP3AmcBdQCq83sEefc+mSFEwmVSFHwlexXCQOdMqqaDhUTkpNBMloiI/AZwBvOubcAzOx+4DxABS6SDN1fJYj0IpFPt4wGtnS7Xhu97SBmtsDMasyspq6uLoHViYhIdyn/eKJzbqFzrto5V11ZWZnq1YmI5IxECnwrMKbb9arobSIikgaJFPhq4BgzG29mhcB84JHkxBIRkf7E/Samc67dzP4H8H8JdiP8rXPu1aQlExGRQ0poP3Dn3GPAY0nKIiIiA5Dhx1gVEZG+qMBFREIqrccDN7M6YHOc/7wC2JHEOD5pWzJPtmwHaFsyVSLbcqRz7iP7Yae1wBNhZjW9HdA8jLQtmSdbtgO0LZkqFduiKRQRkZBSgYuIhFSYCnyh7wBJpG3JPNmyHaBtyVRJ35bQzIGLiMjBwjQCFxGRblTgIiIhlXEFbmZnm9lrZvaGmV3Zy/1FZvZA9P6VZjYu/SljE8O2XGRmdWb2QvTraz5y9sfMfmtm283slT7uNzP7j+h2vmRm09KdMRYxbMccM9vT7fn4n+nOGCszG2NmS81svZm9ambf62WZsDwvsWxLxj83ZlZsZqvM7MXodvxbL8skt7+ccxnzRXBQrDeBo4BC4EVgUo9lvg3cFb08H3jAd+4EtuUi4A7fWWPYljOAacArfdx/DvA4YMBMYKXvzHFuxxzgUd85Y9yWUcC06OUyYFMv/7/C8rzEsi0Z/9xEf86l0csFwEpgZo9lktpfmTYC33+aNudcK9B1mrbuzgPujV5+CDjTzCyNGWMVy7aEgnNuObDrEIucB/zeBZ4HhpnZqPSki10M2xEazrltzrm10csNwAY+ekassDwvsWxLxov+nBujVwuiXz33Eklqf2Vagcdymrb9yzjn2oE9QHla0g1MTKecAy6Ivrx9yMzG9HJ/GMS6rWEwK/oS+HEzm+w7TCyiL8NPJBjxdRe65+UQ2wIheG7MLN/MXgC2A0865/p8TpLRX5lW4Lnmr8A459wJwJMc+MssfqwlOObEx4FfAn/2nKdfZlYK/An4vnOu3neeRPSzLaF4bpxzHc65qQRnKJthZlNSub5MK/BYTtO2fxkziwBDgZ1pSTcw/W6Lc26nc64levVu4KQ0ZUu2rDi9nnOuvuslsAuOdV9gZhWeY/XJzAoICu+PzrmHe1kkNM9Lf9sStufGObcbWAqc3eOupPZXphV4LKdpewT4SvTyhcASF31HIMP0uy095iPPJZj7C6NHgC9H93qYCexxzm3zHWqgzOzwrvlIM5tB8PuRiYMDojkXARucc7f2sVgonpdYtiUMz42ZVZrZsOjlQcBZwMYeiyW1vxI6I0+yuT5O02Zm/w7UOOceIXii7zOzNwjekJrvL3HfYtyW75rZuUA7wbZc5C3wIZjZYoK9ACrMrBb4CcEbNDjn7iI4K9M5wBtAE/BVP0kPLYbtuBD4lpm1A/uA+Rk6OAA4FfgS8HJ0zhXgamAshOt5IbZtCcNzMwq418zyCf7APOicezSV/aWP0ouIhFSmTaGIiEiMVOAiIiGlAhcRCSkVuIhISKnARURCSgUuIhJSKnARkZD6/1f2vIIyH3IDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mode, arr in enumerate(qerrors[:]):\n",
    "    print(len(arr))\n",
    "    plt.plot(np.array(arr), label = '{}'.format(mode))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9985, 0.9990, 0.9997, 0.9995, 0.9990, 0.9976, 0.9994, 0.9985, 0.9959,\n",
      "        1.0010, 0.9991, 0.9980, 0.9979, 0.9984, 0.9975, 0.9989, 0.9985, 0.9994,\n",
      "        0.9993, 0.9990, 0.9991, 0.9987, 1.0006, 0.9991, 0.9988, 0.9982, 0.9997,\n",
      "        0.9992, 0.9990, 1.0019, 0.9993, 0.9980, 1.0001, 0.9999, 0.9995, 0.9988,\n",
      "        0.9996, 1.0010, 0.9973, 0.9995, 0.9990, 1.0020, 0.9988, 0.9991, 0.9987,\n",
      "        0.9969, 0.9982, 0.9987, 0.9979, 0.9977, 0.9992, 1.0007, 1.0003, 1.0012,\n",
      "        0.9974, 0.9987, 0.9994, 0.9974, 1.0014, 0.9990, 0.9976, 1.0005, 0.9998,\n",
      "        0.9978, 0.9986, 0.9992, 0.9990, 0.9980, 0.9973, 0.9984, 0.9999, 0.9994,\n",
      "        0.9971, 1.0007, 1.0003, 0.9998, 0.9983, 0.9984, 0.9983, 1.0002, 1.0004,\n",
      "        0.9991, 0.9983, 0.9971, 0.9992, 1.0003, 0.9984, 0.9981, 0.9992, 0.9985,\n",
      "        0.9997, 0.9976, 1.0001, 0.9996, 1.0001, 1.0003, 0.9978, 0.9995, 1.0005,\n",
      "        1.0009, 1.0013, 0.9972, 0.9983, 0.9974, 0.9970, 0.9981, 1.0006, 0.9989,\n",
      "        0.9996, 0.9971, 0.9990, 0.9976, 0.9988, 1.0002, 1.0020, 0.9992, 0.9997,\n",
      "        1.0009, 1.0001, 1.0011, 0.9993, 0.9992, 0.9996, 0.9975, 0.9988, 1.0002,\n",
      "        0.9998, 0.9999, 1.0014, 0.9988, 0.9974, 0.9979, 0.9987, 0.9974, 1.0009,\n",
      "        0.9993, 1.0003, 0.9976, 0.9989, 1.0008, 0.9983, 1.0005, 1.0008, 0.9979,\n",
      "        0.9995, 0.9979, 0.9975, 0.9981, 0.9981, 0.9982, 0.9970, 0.9994, 0.9968,\n",
      "        0.9984, 0.9989, 0.9978, 0.9983, 0.9996, 0.9991, 0.9986, 0.9976, 0.9983,\n",
      "        0.9985, 0.9981, 0.9955, 1.0000, 1.0000, 0.9983, 0.9988, 0.9968, 0.9997,\n",
      "        0.9975, 0.9996, 1.0004, 0.9993, 0.9995, 0.9991, 0.9988, 0.9989, 0.9987,\n",
      "        0.9973, 0.9970, 0.9994, 0.9995, 1.0019, 1.0003, 0.9988, 0.9978, 0.9991,\n",
      "        0.9990, 1.0003, 0.9998, 0.9976, 0.9972, 0.9991, 0.9986, 0.9980, 0.9989,\n",
      "        0.9991, 1.0007, 1.0002, 1.0003, 1.0003, 0.9980, 0.9984, 1.0007, 0.9969,\n",
      "        0.9998, 0.9990, 0.9995, 0.9990, 0.9981, 0.9984, 0.9988, 1.0008, 1.0015,\n",
      "        0.9972, 1.0008, 0.9991, 0.9987, 0.9971, 0.9987, 1.0009, 0.9997, 0.9966,\n",
      "        0.9978, 0.9975, 1.0003, 1.0003, 0.9991, 1.0008, 0.9989, 0.9999, 1.0017,\n",
      "        0.9996, 1.0013, 1.0003, 0.9989, 0.9993, 0.9973, 1.0006, 0.9992, 0.9998,\n",
      "        1.0004, 0.9997, 1.0004, 0.9996, 0.9989, 0.9994, 0.9977, 0.9994, 0.9973,\n",
      "        0.9976, 1.0005, 0.9978, 0.9980, 0.9975, 0.9988, 0.9994, 0.9994, 0.9989,\n",
      "        0.9997, 1.0005, 1.0008, 0.9998, 0.9969, 0.9987, 0.9995, 0.9971, 0.9967,\n",
      "        1.0002, 0.9973, 1.0004, 0.9977, 0.9998, 1.0007, 0.9980, 0.9995, 0.9989,\n",
      "        0.9966, 1.0008, 1.0009, 0.9998, 1.0004, 0.9991, 0.9978, 0.9979, 0.9979,\n",
      "        0.9990, 0.9976, 0.9985, 1.0005, 0.9987, 0.9988, 0.9976, 0.9982, 0.9987,\n",
      "        0.9979, 0.9983, 0.9994], device='cuda:0')\n",
      "tensor([0.9999, 1.0018, 0.9994, 0.9983, 0.9979, 0.9982, 0.9977, 0.9977, 1.0006,\n",
      "        1.0001, 1.0009, 0.9988, 0.9977, 0.9978, 0.9982, 0.9988, 0.9971, 0.9998,\n",
      "        0.9987, 1.0016, 0.9996, 1.0004, 1.0012, 0.9974, 0.9975, 0.9989, 0.9988,\n",
      "        1.0006, 0.9993, 1.0000, 1.0004, 0.9972, 1.0009, 1.0002, 0.9987, 0.9993,\n",
      "        0.9970, 1.0009, 0.9985, 1.0000, 1.0006, 0.9983, 1.0005, 0.9997, 0.9990,\n",
      "        1.0006, 0.9976, 0.9990, 0.9979, 0.9978, 1.0003, 0.9988, 0.9998, 1.0013,\n",
      "        0.9994, 0.9985, 0.9979, 0.9969, 0.9999, 0.9974, 1.0009, 0.9969, 1.0003,\n",
      "        0.9997, 0.9994, 1.0012, 1.0014, 0.9978, 0.9990, 0.9975, 0.9981, 0.9981,\n",
      "        0.9983, 1.0009, 0.9964, 0.9990, 0.9979, 0.9994, 0.9977, 1.0014, 0.9981,\n",
      "        0.9985, 0.9977, 1.0004, 0.9997, 0.9985, 1.0005, 0.9994, 0.9982, 1.0003,\n",
      "        1.0003, 1.0017, 0.9998, 0.9985, 0.9970, 0.9994, 1.0007, 0.9998, 1.0000,\n",
      "        0.9968, 1.0011, 0.9991, 0.9988, 0.9999, 1.0014, 0.9971, 1.0006, 0.9987,\n",
      "        1.0014, 0.9984, 1.0017, 0.9990, 0.9992, 1.0000, 0.9973, 0.9968, 1.0008,\n",
      "        0.9976, 1.0000, 0.9992, 0.9997, 0.9997, 0.9983, 0.9998, 0.9981, 1.0001,\n",
      "        0.9997, 1.0007, 0.9979, 1.0016, 0.9995, 1.0001, 0.9982, 0.9981, 0.9986,\n",
      "        0.9981, 0.9983, 0.9971, 0.9984, 0.9988, 0.9979, 0.9985, 0.9995, 0.9975,\n",
      "        0.9971, 0.9994, 0.9972, 0.9994, 0.9988, 0.9987, 0.9975, 0.9992, 0.9971,\n",
      "        0.9995, 1.0003, 0.9990, 0.9997, 0.9985, 0.9992, 1.0004, 0.9985, 0.9978,\n",
      "        0.9983, 0.9993, 0.9996, 1.0010, 0.9982, 1.0001, 0.9977, 0.9982, 1.0001,\n",
      "        1.0003, 0.9964, 0.9995, 1.0000, 0.9980, 0.9995, 1.0000, 1.0002, 0.9980,\n",
      "        0.9990, 1.0012, 1.0005, 0.9986, 0.9985, 0.9961, 0.9994, 1.0000, 1.0000,\n",
      "        1.0006, 0.9982, 1.0012, 1.0002, 0.9976, 0.9996, 0.9984, 0.9993, 0.9983,\n",
      "        0.9973, 0.9992, 1.0006, 0.9983, 0.9986, 0.9994, 1.0000, 0.9992, 0.9975,\n",
      "        0.9993, 0.9975, 0.9972, 0.9981, 1.0006, 1.0008, 0.9979, 0.9986, 0.9972,\n",
      "        0.9986, 0.9980, 0.9996, 0.9989, 0.9981, 0.9990, 0.9987, 0.9997, 0.9988,\n",
      "        0.9984, 0.9976, 1.0002, 0.9979, 0.9968, 0.9988, 0.9979, 0.9989, 0.9979,\n",
      "        1.0008, 1.0000, 0.9997, 0.9997, 1.0009, 0.9999, 0.9972, 0.9999, 1.0008,\n",
      "        0.9989, 0.9978, 0.9981, 1.0011, 0.9998, 0.9982, 0.9988, 0.9989, 0.9974,\n",
      "        0.9996, 0.9981, 0.9976, 0.9985, 0.9975, 1.0002, 1.0006, 1.0007, 1.0001,\n",
      "        1.0000, 1.0003, 0.9996, 0.9991, 0.9982, 0.9973, 0.9992, 0.9987, 1.0000,\n",
      "        0.9984, 0.9991, 0.9989, 0.9968, 1.0002, 0.9985, 0.9961, 0.9993, 0.9998,\n",
      "        0.9990, 1.0006, 0.9985, 0.9995, 1.0010, 0.9999, 0.9995, 0.9983, 0.9985,\n",
      "        0.9978, 0.9976, 1.0006, 0.9982, 0.9987, 0.9993, 1.0002, 0.9979, 1.0006,\n",
      "        0.9989, 1.0003, 1.0008], device='cuda:0')\n",
      "tensor([1.0005, 0.9976, 0.9967, 0.9976, 1.0000, 0.9967, 1.0019, 0.9971, 0.9983,\n",
      "        0.9974, 1.0000, 1.0000, 0.9993, 1.0007, 1.0004, 0.9998, 0.9980, 0.9973,\n",
      "        0.9989, 1.0012, 0.9987, 1.0002, 0.9967, 1.0009, 0.9997, 1.0008, 1.0003,\n",
      "        0.9968, 1.0006, 0.9983, 0.9966, 0.9975, 0.9977, 0.9998, 0.9985, 0.9977,\n",
      "        0.9994, 0.9999, 1.0003, 0.9983, 0.9992, 0.9996, 0.9975, 0.9982, 0.9970,\n",
      "        0.9995, 0.9977, 0.9965, 0.9979, 0.9996, 0.9980, 1.0014, 0.9990, 0.9981,\n",
      "        1.0009, 0.9992, 0.9997, 1.0010, 0.9983, 0.9984, 1.0011, 1.0012, 0.9981,\n",
      "        0.9967, 0.9990, 0.9970, 1.0001, 0.9984, 0.9999, 1.0001, 0.9991, 0.9985,\n",
      "        0.9972, 0.9968, 1.0004, 0.9992, 0.9969, 0.9995, 0.9994, 0.9987, 0.9987,\n",
      "        0.9983, 1.0004, 0.9985, 0.9999, 0.9989, 0.9993, 1.0010, 1.0001, 0.9975,\n",
      "        0.9989, 1.0002, 0.9966, 0.9984, 0.9989, 1.0007, 0.9968, 0.9966, 1.0002,\n",
      "        0.9985, 0.9990, 0.9972, 0.9965, 0.9977, 1.0003, 0.9989, 1.0015, 0.9999,\n",
      "        0.9958, 0.9973, 0.9965, 0.9978, 0.9981, 0.9970, 0.9987, 0.9971, 1.0001,\n",
      "        0.9982, 0.9972, 0.9975, 0.9998, 0.9976, 0.9982, 0.9996, 1.0006, 0.9980,\n",
      "        0.9988, 0.9976, 0.9991, 0.9985, 0.9973, 0.9992, 1.0000, 0.9964, 0.9998,\n",
      "        0.9981, 0.9972, 0.9998, 0.9990, 0.9995, 0.9999, 0.9989, 0.9983, 1.0010,\n",
      "        0.9977, 0.9998, 1.0004, 0.9966, 0.9983, 0.9988, 0.9979, 0.9984, 0.9976,\n",
      "        0.9976, 0.9995, 0.9990, 0.9973, 0.9971, 0.9962, 0.9984, 0.9983, 0.9998,\n",
      "        0.9949, 0.9974, 0.9979, 1.0005, 1.0004, 0.9974, 1.0003, 0.9996, 0.9964,\n",
      "        1.0007, 1.0003, 0.9995, 0.9982, 0.9968, 0.9966, 0.9979, 0.9969, 0.9996,\n",
      "        0.9992, 0.9996, 1.0010, 0.9981, 0.9987, 1.0001, 0.9996, 0.9997, 0.9976,\n",
      "        0.9993, 1.0000, 1.0008, 0.9986, 0.9998, 0.9969, 1.0000, 0.9965, 0.9975,\n",
      "        0.9977, 0.9997, 0.9972, 1.0006, 1.0003, 0.9972, 1.0017, 0.9994, 0.9992,\n",
      "        1.0010, 1.0004, 0.9973, 0.9973, 1.0009, 0.9999, 0.9997, 1.0002, 0.9980,\n",
      "        0.9985, 1.0008, 0.9998, 0.9960, 0.9974, 1.0014, 0.9978, 1.0019, 0.9972,\n",
      "        1.0005, 1.0009, 0.9976, 1.0000, 0.9972, 1.0007, 0.9964, 0.9996, 0.9969,\n",
      "        0.9983, 1.0003, 1.0016, 0.9972, 1.0005, 1.0005, 0.9977, 0.9996, 0.9996,\n",
      "        0.9984, 0.9981, 0.9992, 0.9971, 0.9972, 0.9976, 0.9978, 1.0000, 0.9990,\n",
      "        0.9995, 0.9964, 0.9995, 1.0009, 0.9973, 1.0004, 0.9974, 0.9997, 1.0005,\n",
      "        1.0007, 0.9975, 0.9981, 0.9999, 0.9992, 0.9973, 0.9999, 1.0001, 1.0007,\n",
      "        1.0003, 0.9985, 0.9991, 1.0012, 0.9971, 1.0003, 0.9969, 0.9971, 1.0003,\n",
      "        0.9986, 0.9976, 0.9978, 1.0009, 1.0003, 0.9987, 0.9990, 0.9992, 1.0015,\n",
      "        1.0013, 0.9973, 0.9992, 0.9972, 0.9992, 0.9981, 0.9994, 0.9986, 1.0010,\n",
      "        0.9993, 1.0011, 0.9969], device='cuda:0')\n",
      "tensor([152244.7344,  66968.6250, 240070.8594,  87953.1797, 142843.1719,\n",
      "         52450.6211,   4008.2390, 433566.5938, 552303.8750, 136885.5469,\n",
      "         40776.8555, 332069.3750, 195537.6094,  48745.7930,  42192.2070,\n",
      "        177307.5781,   9964.6836,  43027.3789, 251075.7812, 156363.7969,\n",
      "        185005.9844, 105421.2344, 117127.3516, 170715.5625, 157293.7969,\n",
      "        137481.2969,  10734.0195,  51776.3945,  53121.6289, 155888.0938,\n",
      "        130418.2031, 138028.6406,  89075.7656, 188913.9219, 263867.0625,\n",
      "         31212.4590, 134676.5469, 153112.2656,  94716.9531,  31120.0703,\n",
      "         52538.8906, 164121.8438, 170939.6094, 185119.6406, 111293.7891,\n",
      "         90491.1250, 368846.0938,  73672.7344, 186453.0938,  25136.0430,\n",
      "        129521.6172, 111377.5938, 115033.2031,  46910.3242,  62915.4688,\n",
      "        115045.9375, 128681.8125,  45439.1914, 428080.0000,  23785.9824,\n",
      "        153748.8906, 153707.6562,  70396.9141, 120273.5781, 292084.7812,\n",
      "        140013.6562, 111287.4531,  11986.1504, 253066.2188, 255223.1406,\n",
      "         38172.6094, 239022.9219, 103235.4844,  93489.7812, 160337.1875,\n",
      "        239926.9219, 110738.0078, 134817.2656,  73806.9766, 136308.2812,\n",
      "         89135.1562, 178921.8750,  80682.3828, 231011.8438, 158708.4844,\n",
      "        242886.0625, 127865.0391,  78522.4844,  56090.6211, 282265.8125,\n",
      "        544567.1250, 106431.0547,  48970.8555, 155728.4062, 159627.9062,\n",
      "        205265.0156, 266681.7188, 368339.4062,  25385.6133,  87701.4609,\n",
      "        223782.3906,  86224.9688, 210344.7812, 166721.4688,  36626.7031,\n",
      "        273003.3750, 158300.5938, 234640.7344, 129204.6641,  60264.5312,\n",
      "         16222.9990,  44258.5078,  65070.8359,  94949.4375,  97556.9062,\n",
      "        178015.1406,  52261.3750,  42998.6250, 106209.0391, 157452.8281,\n",
      "         58469.2969, 348632.1250, 121573.6562, 404097.6875, 108760.0938,\n",
      "        284940.0625,  25742.4707, 135547.3438, 203289.3281,  11812.1846,\n",
      "        130376.7109, 184557.2031,  64234.8438, 396805.6875,  40276.0352,\n",
      "        269571.7188, 132737.9219, 154101.8594,  18308.5234,  57213.1055,\n",
      "        106321.0781,  86960.7656, 137542.5156, 181736.8906,  91520.7812,\n",
      "         58240.4766,  87800.8125, 129447.4922, 246430.2656, 135464.6719,\n",
      "        112095.8438, 198018.3438, 129399.4062, 116191.6172, 117867.6719,\n",
      "        139401.0469, 147017.8750, 193210.4062, 194900.6406, 137777.7188,\n",
      "        350642.5938,  43089.4219, 183332.4688,  63583.9023, 222553.6094,\n",
      "         98791.4609, 105884.7266,  36041.5352, 283422.1875,  54352.2070,\n",
      "         58407.0156, 101566.7188,  57457.0234,  48485.9375,  14685.9326,\n",
      "        152177.9062, 164767.8125,  50403.6992, 225981.4219,  58663.1016,\n",
      "         50361.9570,  71645.7812,  61266.9922,  70978.6641, 172551.1719,\n",
      "         56867.0898,  20409.0566, 102248.2812, 191652.1250,  59757.8555,\n",
      "        101685.2969,  79160.3281, 197848.3906, 256627.5781,  68269.8047,\n",
      "        200534.8125,  43443.4805, 241819.1562, 103057.6562, 213862.7969,\n",
      "         32213.2949,   6850.4321,  55697.2227, 309698.2188, 163438.4219,\n",
      "        135024.6875, 134292.7500,  41289.5664, 100045.7578, 313379.4688,\n",
      "        138555.0312, 160786.8750,  79513.6406, 263510.2812,  78038.6719,\n",
      "         80177.6484,  25930.9824, 295297.7500,  64971.5781,  39803.4531,\n",
      "         49508.8867, 378241.9375,  69039.5000, 180832.0938, 417888.5625,\n",
      "        112406.0547, 449476.5938,  22539.0723, 217651.2812, 259993.1094,\n",
      "         17613.9570,  12060.9570, 162679.8906, 142793.6562,  11461.7930,\n",
      "         40055.2656, 217734.0625, 152593.0000, 209651.6250,  59913.8086,\n",
      "         21099.9746,  97634.6094,  61486.3047,  91908.3594,  21139.0469,\n",
      "        106325.9375,  99390.0859, 114964.5312, 240814.9062,   9092.2949,\n",
      "        461204.5938, 214832.3281, 256579.6250, 184131.6562,  47945.8906,\n",
      "         54352.6953,  15747.8203,  24814.1484, 169357.5469, 130115.0156,\n",
      "        193997.6250,  48655.8867,  86017.1953, 130054.4922, 142418.1875,\n",
      "         66953.9062,  26190.5078,  92469.7422, 223859.7188, 125229.3516,\n",
      "         46828.5977,  71596.7109, 158676.7812, 167324.5938,  30483.6426,\n",
      "         61328.6680, 115067.4141, 103394.5078,   3301.8826, 511793.7812,\n",
      "        232346.7969,  34583.2734,  60921.9375, 409215.8750,  25157.0234,\n",
      "        174612.1094, 112438.4766, 220684.9688,  26191.9785, 139078.2344,\n",
      "         74430.5312, 151317.6875, 201599.6719, 150994.8750, 130442.9062,\n",
      "        164066.1562, 128304.8047, 292573.3750,  92500.3906, 294721.5312],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tl.norm(qout.factors[i], 2, axis = 0))\n",
    "    \n",
    "    \n",
    "print(qout.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_factors(factors_flatten, shapes):\n",
    "    factors_restored = []\n",
    "    l = 0\n",
    "    for shape in shapes:\n",
    "        dl = np.prod(shape)\n",
    "        factors_restored.append(torch.tensor(factors_flatten)[l : l + dl].reshape(shape))\n",
    "        l += dl\n",
    "    return factors_restored\n",
    "\n",
    "\n",
    "def norm_error(factors_flatten, shapes, t, weights = None,\\\n",
    "               scales = None, zero_points = None):\n",
    "    \n",
    "    factors_restored = restore_factors(factors_flatten, shapes)\n",
    "    if scales is not None:\n",
    "        factors_restored = [(factors_restored[i] - zero_points[i]) * scales[i]\\\n",
    "                            for i in range(len(scales))]\n",
    "\n",
    "    tensor_restored = kruskal_to_tensor(KruskalTensor((weights, factors_restored)))\n",
    "\n",
    "    norm_error = tl.norm(tensor_restored - t)\n",
    "  \n",
    "    global i\n",
    "    global gnorm\n",
    "    i += 1\n",
    "    gnorm = float(norm_error)\n",
    "    \n",
    "    return float(norm_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS + post quantization + simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmin=-128\n",
    "qmax=127\n",
    "qfactors_post[0]/scales_post[0] + zero_points_post[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_post[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qmin = -128\n",
    "# qmax = 127\n",
    "qfactors_post_int = [(qfactors_post[i]/scales_post[i] + zero_points_post[i]).cpu() \\\n",
    "                     for i in range(len(qfactors_post))]\n",
    "\n",
    "shapes = [factor.shape for factor in qfactors_post_int]\n",
    "factors_flatten_init =  torch.cat([factor.flatten() for factor in qfactors_post_int])\n",
    "factors_flatten_init = factors_flatten_init.type(torch.int8)\n",
    "\n",
    "\n",
    "bsa_als = None\n",
    "\n",
    "# numpy_to_struct = {np.float32 : 'f', np.float64 : 'd',  np.float16 : 'e',\\\n",
    "#                    np.int8 : 'b', np.int16 : 'h', np.int32 : 'i', np.int64 : 'q',\\\n",
    "#                    np.uint8 : 'B', np.uint16 : 'H', np.uint32 : 'I', np.uint64 : 'Q'}\n",
    "\n",
    "\n",
    "mask = np.zeros(8).astype(bool)\n",
    "mask[-1:] = True\n",
    "\n",
    "neighbor_params = {'nb':1, 'mask': mask}\n",
    "neighbor = p.sa.MaskedBitsNeighbor(**neighbor_params)\n",
    "\n",
    "i = 0\n",
    "gnorm = 0\n",
    "bsa_als = p.BinarySA(partial(norm_error,\n",
    "                             shapes = shapes,\n",
    "                             t = t.cpu(),\n",
    "                             weights = weights_post.cpu(),\n",
    "                             scales = [s.cpu() for s in scales_post],\n",
    "                             zero_points = [z.cpu() for z in zero_points_post]\n",
    "                            ),\n",
    "                             np.array(factors_flatten_init).astype(np.int8),\n",
    "                             [(-2**7, 2**7-1)]*len(factors_flatten_init),\n",
    "                             'i'*len(factors_flatten_init),\n",
    "                             emax = 1e-10,\n",
    "                             imax = 10000,\n",
    "                             T0 = 1,\n",
    "                             rt = 0.8,\n",
    "                             neighbor = neighbor\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sense to run several times\n",
    "for r in range(10000):\n",
    "    flatten_factors_opt_als, error_als =  bsa_als()\n",
    "    if r % 100 == 0:\n",
    "        print('round {}, i = {}, norm_error = {}'.format(r, i, gnorm))\n",
    "    i = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_factors_opt_als, error_als =  bsa_als()\n",
    "print(gnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALS + simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qout_factors_int = [(qout.factors[i].cpu()/scales[i]+zero_points[i]).cpu() for i in range(len(qout.factors))]\n",
    "\n",
    "shapes = [factor.shape for factor in qout_factors_int]\n",
    "factors_flatten_init =  torch.cat([factor.flatten() for factor in qout_factors_int])\n",
    "factors_flatten_init = factors_flatten_init.type(torch.int8)\n",
    "\n",
    "\n",
    "bsa_qals = None\n",
    "\n",
    "# numpy_to_struct = {np.float32 : 'f', np.float64 : 'd',  np.float16 : 'e',\\\n",
    "#                    np.int8 : 'b', np.int16 : 'h', np.int32 : 'i', np.int64 : 'q',\\\n",
    "#                    np.uint8 : 'B', np.uint16 : 'H', np.uint32 : 'I', np.uint64 : 'Q'}\n",
    "\n",
    "\n",
    "mask = np.zeros(8).astype(bool)\n",
    "mask[-1:] = True\n",
    "\n",
    "neighbor_params = {'nb':1, 'mask': mask}\n",
    "neighbor = p.sa.MaskedBitsNeighbor(**neighbor_params)\n",
    "\n",
    "bsa_qals = p.BinarySA(partial(norm_error,\n",
    "                             shapes = shapes,\n",
    "                             t = t.cpu(),\n",
    "                             weights = weights_post.cpu(),\n",
    "                             scales = [s.cpu() for s in scales_post],\n",
    "                             zero_points = [z.cpu() for z in zero_points_post]\n",
    "                            ),\n",
    "                             np.array(factors_flatten_init).astype(np.int8),\n",
    "                             [(-2**7, 2**7-1)]*len(factors_flatten_init),\n",
    "                             'i'*len(factors_flatten_init),\n",
    "                             emax = 1e-10,\n",
    "                             imax = 10000,\n",
    "                             T0 = 1,\n",
    "                             rt = 0.8,\n",
    "                             neighbor = neighbor\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sense to run several times\n",
    "for r in range(10000):\n",
    "    flatten_factors_opt_qals, error_qals =  bsa_qals()\n",
    "    if r % 100 == 0:\n",
    "        print('round {}, i = {}, norm_error = {}'.format(r, i, gnorm))\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_factors_opt_qals, error_qals =  bsa_qals()\n",
    "print(gnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_factors_opt_qals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
