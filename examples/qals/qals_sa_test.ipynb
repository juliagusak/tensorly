{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import peach as p\n",
    "\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "\n",
    "from tensorly.decomposition import quantized_parafac\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_struct = {np.float32 : 'f', np.int8 : 'i'}\n",
    "pytorch_to_struct = {torch.float32 : 'f', torch.int8 : 'i'}\n",
    "\n",
    "N_ITER_MAX = 10000\n",
    "RANDOM_INIT_STARTS = 10\n",
    "\n",
    "\n",
    "params_als_shared = {'n_iter_max' : N_ITER_MAX,\\\n",
    "                    'stop_criterion' : 'rec_error_decrease',\\\n",
    "                    'tol' : None,\\\n",
    "                    'normalize_factors' : True,\\\n",
    "                    'svd' : 'numpy_svd',\\\n",
    "                    'verbose' : 0}\n",
    "\n",
    "params_als = {'orthogonalise' : False,\\\n",
    "              'non_negative' : False,\\\n",
    "              'mask' : None,\\\n",
    "              'return_errors' : False}\n",
    "\n",
    "params_qint = {'dtype' : torch.qint8,\\\n",
    "               'qscheme' : torch.per_tensor_affine,\\\n",
    "               'dim' : None,\\\n",
    "               'return_scale_zeropoint' : True}\n",
    "\n",
    "params_qals = {'qmodes' : [0, 1, 2],\\\n",
    "               'warmup_iters' : 1,\\\n",
    "               'return_scale_zeropoint' : True,\\\n",
    "               'return_rec_errors' : False,\\\n",
    "               'return_qrec_errors' : False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tensor in Kruskal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||factors||: 10141.3798828125, factors type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "rank = 2\n",
    "shape = (5, 5, 3)\n",
    "\n",
    "dtype =  torch.float32\n",
    "struct_dtype = pytorch_to_struct[dtype]\n",
    "\n",
    "factors = get_factors_init(shape, rank, dtype = struct_dtype)\n",
    "weights = torch.ones(rank).type(dtype)\n",
    "\n",
    "krt = KruskalTensor((weights, factors))\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t.type(torch.float32))\n",
    "print('||factors||: {}, factors type: {}'.format(tnorm, t.dtype))                                                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_init_starts = RANDOM_INIT_STARTS\n",
    "inits = ['svd'] + ['random'] * random_init_starts\n",
    "random_states = [None] + [int(torch.randint(high = 11999, size = (1,))) for _ in range(random_init_starts)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 2\n",
    "\n",
    "run_id = 0\n",
    "init = inits[run_id]\n",
    "random_state = random_states[run_id]\n",
    "\n",
    "out_als = parafac(t, rank,\\\n",
    "              init=init,\\\n",
    "              random_state=random_state,\\\n",
    "              **params_als_shared,\\\n",
    "              **params_als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||tensor - als_factors||: 0.0013729545753449202\n"
     ]
    }
   ],
   "source": [
    "t_als = kruskal_to_tensor(out_als) \n",
    "rec_error_als = tl.norm(t - t_als)\n",
    "\n",
    "print('||tensor - als_factors||: {}'.format(rec_error_als))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS + post quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||tensor - als_factors_quantized||: 1934.88818359375\n"
     ]
    }
   ],
   "source": [
    "qfactors_post = []\n",
    "scales_post = []\n",
    "zero_points_post = []\n",
    "\n",
    "for i in range(len(out_als.factors)):\n",
    "    q, s, z = quantize_qint(out_als.factors[i], **params_qint)\n",
    "    qfactors_post.append(q)\n",
    "    scales_post.append(s)\n",
    "    zero_points_post.append(z)\n",
    "\n",
    "qt_post = kruskal_to_tensor(KruskalTensor((out_als.weights, qfactors_post)))\n",
    "rec_error_qpost = tl.norm(t - qt_post)\n",
    "\n",
    "print('||tensor - als_factors_quantized||: {}'.format(rec_error_qpost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40., -128.],\n",
       "        [-128.,   90.],\n",
       "        [ -15.,  100.],\n",
       "        [  51.,  -48.],\n",
       "        [-128.,   54.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qfactors_post[0]/scales_post[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||tensor - qals_factors||: 1631.13037109375\n"
     ]
    }
   ],
   "source": [
    "(fout, qout), (errors, qerrors), scales, zero_points = quantized_parafac(t, rank,\\\n",
    "                                                                         init=init,\\\n",
    "                                                                         random_state=random_state,\\\n",
    "                                                                         **params_als_shared,\\\n",
    "                                                                         **params_qals,\\\n",
    "                                                                         **params_qint)\n",
    "\n",
    "t_approx_qals = kruskal_to_tensor(KruskalTensor(qout))\n",
    "rec_error_qals = tl.norm(t - t_approx_qals)\n",
    "\n",
    "print('||tensor - qals_factors||: {}'.format(rec_error_qals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -33.0000, -127.0000],\n",
       "         [ 123.0000,   84.0000],\n",
       "         [  12.0000,   81.0000],\n",
       "         [ -43.0000,  -43.0000],\n",
       "         [ 127.0000,   55.0000]]), tensor([7097.9902, 6765.4238]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qout.factors[0]/scales[0], qout.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_factors(factors_flatten, shapes):\n",
    "    factors_restored = []\n",
    "    l = 0\n",
    "    for shape in shapes:\n",
    "        dl = np.prod(shape)\n",
    "        factors_restored.append(torch.tensor(factors_flatten)[l : l + dl].reshape(shape))\n",
    "        l += dl\n",
    "    return factors_restored\n",
    "\n",
    "def norm_error(factors_flatten, shapes, t, weights = None,\\\n",
    "               scales = None, zero_points = zero_points):\n",
    "    \n",
    "    factors_restored = restore_factors(factors_flatten, shapes)\n",
    "    if scales is not None:\n",
    "        factors_restored = [factors_restored[i]*scales[i] + zero_points[0] for i in range(len(scales))]\n",
    "        \n",
    "    tensor_restored = kruskal_to_tensor(KruskalTensor((weights, factors_restored)))\n",
    "\n",
    "    tensor_restored.shape\n",
    "\n",
    "    norm_error = tl.norm(tensor_restored - t)\n",
    "    \n",
    "    print('Norm error: {}'.format(norm_error))\n",
    "    \n",
    "    return float(norm_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS + post quantization + simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  40., -128.],\n",
       "         [-128.,   90.],\n",
       "         [ -15.,  100.],\n",
       "         [  51.,  -48.],\n",
       "         [-128.,   54.]]),\n",
       " [tensor(0.0046), tensor(0.0059), tensor(0.0070)],\n",
       " (tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32)),\n",
       " tensor([7182.6895, 6582.5713]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qfactors_post[0]/scales_post[0], scales_post, zero_points, out_als.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "qfactors_post_int = [qfactors_post[i]/scales_post[i]\\\n",
    "                     for i in range(len(qfactors_post))]\n",
    "\n",
    "shapes = [factor.shape for factor in qfactors_post_int]\n",
    "factors_flatten_init =  torch.cat([factor.flatten() for factor in qfactors_post_int])\n",
    "factors_flatten_init = factors_flatten_init.type(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm error: 1934.88818359375\n"
     ]
    }
   ],
   "source": [
    "bsa = None\n",
    "\n",
    "# numpy_to_struct = {np.float32 : 'f', np.float64 : 'd',  np.float16 : 'e',\\\n",
    "#                    np.int8 : 'b', np.int16 : 'h', np.int32 : 'i', np.int64 : 'q',\\\n",
    "#                    np.uint8 : 'B', np.uint16 : 'H', np.uint32 : 'I', np.uint64 : 'Q'}\n",
    "\n",
    "\n",
    "mask = np.zeros(8).astype(bool)\n",
    "mask[-1] = True\n",
    "\n",
    "neighbor_params = {'nb':2, 'mask': mask}\n",
    "neighbor = p.sa.MaskedInvertBitsNeighbor(**neighbor_params)\n",
    "\n",
    "bsa = p.BinarySA(partial(norm_error,\\\n",
    "                         shapes = shapes, t = t, weights = out_als.weights,\\\n",
    "                         scales = scales_post, zero_points = zero_points_post),\\\n",
    "                 np.array(factors_flatten_init).astype(np.int8),\\\n",
    "#                  np.array(flatten_factors_opt),\\\n",
    "                 [(-2**32, 2**32)]*len(factors_flatten_init),\\\n",
    "                 'f'*len(factors_flatten_init),\\\n",
    "                  emax = 1e-12,\\\n",
    "                  imax = 50000,\\\n",
    "                  T0 = 1000,\\\n",
    "                  rt = 0.999, neighbor = neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm error: 1934.88818359375\n",
      "Norm error: 10966.8486328125\n",
      "Norm error: 1934.88818359375\n",
      "Norm error: 1934.877197265625\n",
      "Norm error: 1934.877197265625\n",
      "Norm error: 1934.876708984375\n",
      "Norm error: 1934.876708984375\n",
      "Norm error: 5176.64990234375\n",
      "Norm error: 1934.876708984375\n",
      "Norm error: 6046.2802734375\n",
      "Norm error: 1934.876708984375\n",
      "Norm error: 1934.8243408203125\n",
      "Norm error: 1934.8243408203125\n",
      "Norm error: 1934.810302734375\n",
      "Norm error: 1934.810302734375\n",
      "Norm error: 5025.10595703125\n",
      "Norm error: 1934.810302734375\n",
      "Norm error: 12302.564453125\n",
      "Norm error: 1934.810302734375\n",
      "Norm error: 1931.247802734375\n",
      "Norm error: 1931.247802734375\n",
      "Norm error: 4149.4375\n",
      "Norm error: 1931.247802734375\n",
      "Norm error: 1931.0098876953125\n",
      "Norm error: 1931.0098876953125\n",
      "Norm error: 4138.51025390625\n",
      "Norm error: 1931.0098876953125\n",
      "Norm error: 6141.02294921875\n",
      "Norm error: 1931.0098876953125\n",
      "Norm error: 1931.0045166015625\n",
      "Norm error: 1931.0045166015625\n",
      "Norm error: 10974.037109375\n",
      "Norm error: 1931.0045166015625\n",
      "Norm error: 1931.00146484375\n",
      "Norm error: 1931.00146484375\n",
      "Norm error: 1930.9818115234375\n",
      "Norm error: 1930.9818115234375\n",
      "Norm error: 1930.1649169921875\n",
      "Norm error: 1930.1649169921875\n",
      "Norm error: 1929.412353515625\n",
      "Norm error: 1929.412353515625\n",
      "Norm error: 10984.9677734375\n",
      "Norm error: 1929.412353515625\n",
      "Norm error: 10984.9658203125\n",
      "Norm error: 1929.412353515625\n",
      "Norm error: 1929.412353515625\n"
     ]
    }
   ],
   "source": [
    "flatten_factors_opt, error =  bsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALS + simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -33.0000, -127.0000],\n",
       "         [ 123.0000,   84.0000],\n",
       "         [  12.0000,   81.0000],\n",
       "         [ -43.0000,  -43.0000],\n",
       "         [ 127.0000,   55.0000]]),\n",
       " (tensor(0.0054), tensor(0.0050), tensor(0.0071)),\n",
       " (tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32)),\n",
       " tensor([7097.9902, 6765.4238]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qout.factors[0]/scales[0], scales, zero_points, qout.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "qout_factors_int = [qout.factors[i]/scales[i] for i in range(len(qout.factors))]\n",
    "\n",
    "shapes = [factor.shape for factor in qout_factors_int]\n",
    "factors_flatten_init =  torch.cat([factor.flatten() for factor in qout_factors_int])\n",
    "factors_flatten_init = factors_flatten_init.type(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm error: 1634.4222412109375\n"
     ]
    }
   ],
   "source": [
    "bsa = None\n",
    "\n",
    "# numpy_to_struct = {np.float32 : 'f', np.float64 : 'd',  np.float16 : 'e',\\\n",
    "#                    np.int8 : 'b', np.int16 : 'h', np.int32 : 'i', np.int64 : 'q',\\\n",
    "#                    np.uint8 : 'B', np.uint16 : 'H', np.uint32 : 'I', np.uint64 : 'Q'}\n",
    "\n",
    "\n",
    "mask = np.zeros(8).astype(bool)\n",
    "mask[-1] = True\n",
    "\n",
    "neighbor_params = {'nb':2, 'mask': mask}\n",
    "neighbor = p.sa.MaskedInvertBitsNeighbor(**neighbor_params)\n",
    "\n",
    "bsa = p.BinarySA(partial(norm_error,\\\n",
    "                         shapes = shapes, t = t, weights = qout.weights,\\\n",
    "                         scales = scales, zero_points = zero_points),\\\n",
    "                 np.array(factors_flatten_init).astype(np.int8),\\\n",
    "#                  np.array(flatten_factors_opt),\\\n",
    "                 [(-2**32, 2**32)]*len(factors_flatten_init),\\\n",
    "                 'f'*len(factors_flatten_init),\\\n",
    "                  emax = 1e-12,\\\n",
    "                  imax = 50000,\\\n",
    "                  T0 = 1000,\\\n",
    "                  rt = 0.999, neighbor = neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm error: 1634.4222412109375\n",
      "Norm error: 1627.099365234375\n",
      "Norm error: 1627.099365234375\n",
      "Norm error: 1627.099365234375\n"
     ]
    }
   ],
   "source": [
    "flatten_factors_opt, error =  bsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
