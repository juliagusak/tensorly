{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, quantized_parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Tensor quantization\n",
    "Quantization scheme can be either affine or symmetric.\n",
    "\n",
    "Scale and zero_point values to perform quantization are computed either per channel or per tensor (i.e. we get either vectors or scalars).\n",
    "\n",
    "Thus, there are 4 types of quantization scheme:\n",
    "\n",
    "    ``torch.per_tensor_affine``\n",
    "    ``torch.per_tensor_symmetric``\n",
    "    ``torch.per_channel_affine``\n",
    "    ``torch.per_channel_symmetric``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_tensor|| = 767.585693359375\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(256, 256, 9)\n",
    "print('||float_tensor|| = {}'.format(tl.norm(t)))\n",
    "\n",
    "dtype = torch.qint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per channel  quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_affine\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.277091026306152\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.280121803283691\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.608396053314209\n",
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_symmetric\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.277091026306152\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.280121803283691\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.608396053314209\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_channel_affine, torch.per_channel_symmetric]:\n",
    "    print(\"\\nPer channel quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "    \n",
    "    for dim in range(len(t.shape)):\n",
    "        qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                              dtype,\\\n",
    "                                              qscheme,\\\n",
    "                                              dim = dim,\\\n",
    "                                              return_scale_zeropoint=True)\n",
    "\n",
    "        print('Per dim {}, ||float_tensor - quant_tensor|| = {}'.format(dim, tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per tensor quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_affine\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.38266372680664\n",
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_symmetric\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.38266372680664\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    print(\"\\nPer tensor quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "\n",
    "    \n",
    "    qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                          dtype,\\\n",
    "                                          qscheme,\\\n",
    "                                          dim = dim,\\\n",
    "                                          return_scale_zeropoint=True)\n",
    "    print('Per tensor, ||float_tensor - quant_tensor|| = {}'.format(tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2. Quantization of a tensor in Kruskal format:\n",
    "    a) via quantization of the corresponding full tensor\n",
    "    b) via quantization of decomposition factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 737.1425170898438\n"
     ]
    }
   ],
   "source": [
    "rank = 16\n",
    "shape = (64, 64, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose quantization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Quantize the full tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors - float_factors_quantized|| = 10.051530838012695\n"
     ]
    }
   ],
   "source": [
    "t_quant = quantize_qint(t, dtype, qscheme, dim = dim)\n",
    "print('||float_factors - float_factors_quantized|| = {}'.format(tl.norm(t - t_quant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Quantize several factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 9.89563274383545\n",
      "\n",
      "[2/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 13.896100044250488\n",
      "\n",
      "[3/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 17.108083724975586\n"
     ]
    }
   ],
   "source": [
    "num_factors = len(factors)\n",
    "for num_quant_factors in range(1, num_factors + 1):\n",
    "    \n",
    "    qfactors = [quantize_qint(factors[i], dtype, qscheme, dim = dim)\\\n",
    "                for i in range(num_quant_factors)\\\n",
    "               ] + [factors[i] for i in range(num_quant_factors, num_factors)]\n",
    "\n",
    "    qkrt = KruskalTensor((weights, qfactors))\n",
    "    qt = kruskal_to_tensor(qkrt)\n",
    "    print('\\n[{}/{}] factors are quantized'.format(num_quant_factors, num_factors))\n",
    "    print('||quant_factors - float_factors|| = {}'.format(tl.norm(qt - t)))\n",
    "#     print('||quant_factors - float_factors_quantized|| = {}'.format(tl.norm(qt - t_quant)))\n",
    "\n",
    "#     qt_quant = quantize_qint(qt, dtype, qscheme, dim = dim)\n",
    "#     print('\\nquant_factors_quantized - t_quant_factors: {}'.format(tl.norm(qt_quant - qt)/tnorm))\n",
    "    \n",
    "#     print('||quant_factors_quantized - float_factors|| = {}'.format(tl.norm(qt_quant - t)/tnorm))\n",
    "#     print('||quant_factors_quantized - float_factors_quantized|| = {}'.format(tl.norm(qt_quant - t_quant)/tnorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3. Quantized ALS\n",
    "Compare standard ALS algorithm for finding  CP decomposition with its quantized version, when at the end of each ALS step approximated factor is quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 992.4262084960938\n"
     ]
    }
   ],
   "source": [
    "rank = 8\n",
    "shape = (128, 128, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 992.4262084960938\n",
      "parafac has stopped after iteration 63\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = True\n",
    "(factors_als, weights_als), _ =  quantized_parafac(t, rank, n_iter_max=50000,\\\n",
    "                                    init='random', tol=1e-8, svd = None,\n",
    "                                    normalize_factors = normalize_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0004)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_als, factors_als))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([391.0013, 355.6662, 269.9358, 423.7779, 310.8251, 315.8976, 267.2900,\n",
       "        430.1736])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using quantized ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 992.4262084960938\n",
      "iteration 500, diff_from_norm = 17.587434768676758, rel_rec_error = 0.017721654887901908\n",
      "iteration 1000, diff_from_norm = 17.587434768676758, rel_rec_error = 0.017721654887901908\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = True\n",
    "(factors_qals, weights_qals), _, scales, zero_points = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init='random', tol= None, svd = None,\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1],\n",
    "                                    quantize_every = 2,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5874)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_qals, factors_qals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([312.2208, 430.0728, 315.8293, 359.4211, 267.2516, 391.3284, 423.4779,\n",
       "        271.4605])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_qals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.0021), tensor(0.0021), None),\n",
       " (tensor(0, dtype=torch.int32), tensor(0, dtype=torch.int32), None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales, zero_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1367,  0.1035,  0.0601,  ...,  0.0352, -0.0518,  0.0911],\n",
       "        [ 0.0911,  0.1139,  0.0083,  ..., -0.0393, -0.2340, -0.1470],\n",
       "        [ 0.0166, -0.1346, -0.1077,  ..., -0.0476,  0.0145, -0.0041],\n",
       "        ...,\n",
       "        [-0.1035,  0.0021, -0.0456,  ...,  0.1346, -0.1160,  0.0704],\n",
       "        [ 0.0725,  0.0414,  0.0393,  ...,  0.0766,  0.0311, -0.0683],\n",
       "        [ 0.0456, -0.0663,  0.0725,  ...,  0.1077, -0.0228,  0.0704]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -66.,   50.,   29.,  ...,   17.,  -25.,   44.],\n",
       "        [  44.,   55.,    4.,  ...,  -19., -113.,  -71.],\n",
       "        [   8.,  -65.,  -52.,  ...,  -23.,    7.,   -2.],\n",
       "        ...,\n",
       "        [ -50.,    1.,  -22.,  ...,   65.,  -56.,   34.],\n",
       "        [  35.,   20.,   19.,  ...,   37.,   15.,  -33.],\n",
       "        [  22.,  -32.,   35.,  ...,   52.,  -11.,   34.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]/scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -43.0000,  -20.0000,  -18.0000,  ...,  -94.0000,   -2.0000,\n",
       "            1.0000],\n",
       "        [  94.0000,   19.0000,   -1.0000,  ...,   23.0000,   74.0000,\n",
       "          -34.0000],\n",
       "        [  -3.0000,    8.0000,  -23.0000,  ...,  -36.0000, -107.0000,\n",
       "           37.0000],\n",
       "        ...,\n",
       "        [  59.0000,   38.0000,   10.0000,  ...,   14.0000,   -1.0000,\n",
       "          -54.0000],\n",
       "        [  -5.0000,   21.0000,   84.0000,  ...,   38.0000,  -10.0000,\n",
       "           71.0000],\n",
       "        [ -11.0000,  -23.0000,  108.0000,  ...,   -8.0000,  -16.0000,\n",
       "           60.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[1]/scales[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
