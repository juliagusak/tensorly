{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, quantized_parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Tensor quantization\n",
    "Quantization scheme can be either affine or symmetric.\n",
    "\n",
    "Scale and zero_point values to perform quantization are computed either per channel or per tensor (i.e. we get either vectors or scalars).\n",
    "\n",
    "Thus, there are 4 types of quantization scheme:\n",
    "\n",
    "    ``torch.per_tensor_affine``\n",
    "    ``torch.per_tensor_symmetric``\n",
    "    ``torch.per_channel_affine``\n",
    "    ``torch.per_channel_symmetric``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_tensor|| = 768.3004150390625\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(256, 256, 9)\n",
    "print('||float_tensor|| = {}'.format(tl.norm(t)))\n",
    "\n",
    "dtype = torch.qint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per channel  quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_affine\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.091872692108154\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.260591983795166\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.5831427574157715\n",
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_symmetric\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.091872692108154\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.260591983795166\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.5831427574157715\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_channel_affine, torch.per_channel_symmetric]:\n",
    "    print(\"\\nPer channel quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "    \n",
    "    for dim in range(len(t.shape)):\n",
    "        qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                              dtype,\\\n",
    "                                              qscheme,\\\n",
    "                                              dim = dim,\\\n",
    "                                              return_scale_zeropoint=True)\n",
    "\n",
    "        print('Per dim {}, ||float_tensor - quant_tensor|| = {}'.format(dim, tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per tensor quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_affine\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.514548301696777\n",
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_symmetric\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.514548301696777\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    print(\"\\nPer tensor quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "\n",
    "    \n",
    "    qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                          dtype,\\\n",
    "                                          qscheme,\\\n",
    "                                          dim = dim,\\\n",
    "                                          return_scale_zeropoint=True)\n",
    "    print('Per tensor, ||float_tensor - quant_tensor|| = {}'.format(tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2. Quantization of a tensor in Kruskal format:\n",
    "    a) via quantization of the corresponding full tensor\n",
    "    b) via quantization of decomposition factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 710.7957763671875\n"
     ]
    }
   ],
   "source": [
    "rank = 16\n",
    "shape = (64, 64, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose quantization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Quantize the full tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors - float_factors_quantized|| = 9.844650268554688\n"
     ]
    }
   ],
   "source": [
    "t_quant = quantize_qint(t, dtype, qscheme, dim = dim)\n",
    "print('||float_factors - float_factors_quantized|| = {}'.format(tl.norm(t - t_quant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Quantize several factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 5.352328777313232\n",
      "\n",
      "[2/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 7.942391872406006\n",
      "\n",
      "[3/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 18.10138702392578\n"
     ]
    }
   ],
   "source": [
    "num_factors = len(factors)\n",
    "for num_quant_factors in range(1, num_factors + 1):\n",
    "    \n",
    "    qfactors = [quantize_qint(factors[i], dtype, qscheme, dim = dim)\\\n",
    "                for i in range(num_quant_factors)\\\n",
    "               ] + [factors[i] for i in range(num_quant_factors, num_factors)]\n",
    "\n",
    "    qkrt = KruskalTensor((weights, qfactors))\n",
    "    qt = kruskal_to_tensor(qkrt)\n",
    "    print('\\n[{}/{}] factors are quantized'.format(num_quant_factors, num_factors))\n",
    "    print('||quant_factors - float_factors|| = {}'.format(tl.norm(qt - t)))\n",
    "#     print('||quant_factors - float_factors_quantized|| = {}'.format(tl.norm(qt - t_quant)))\n",
    "\n",
    "#     qt_quant = quantize_qint(qt, dtype, qscheme, dim = dim)\n",
    "#     print('\\nquant_factors_quantized - t_quant_factors: {}'.format(tl.norm(qt_quant - qt)/tnorm))\n",
    "    \n",
    "#     print('||quant_factors_quantized - float_factors|| = {}'.format(tl.norm(qt_quant - t)/tnorm))\n",
    "#     print('||quant_factors_quantized - float_factors_quantized|| = {}'.format(tl.norm(qt_quant - t_quant)/tnorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3. Quantized ALS\n",
    "Compare standard ALS algorithm for finding  CP decomposition with its quantized version, when at the end of each ALS step approximated factor is quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 896.6329956054688\n"
     ]
    }
   ],
   "source": [
    "rank = 8\n",
    "shape = (128, 128, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007)\n",
      "tensor(0.0628)\n"
     ]
    }
   ],
   "source": [
    "for stop_criterion in ['rec_error_decrease', 'rec_error_deviation']:\n",
    "    out = parafac(t, rank, n_iter_max=100, init='svd', svd='numpy_svd',\\\n",
    "                normalize_factors=False, orthogonalise=False,\\\n",
    "                tol=1e-8, random_state=None,\\\n",
    "                verbose=0, return_errors=False,\\\n",
    "                non_negative=False, mask=None,\n",
    "                stop_criterion = stop_criterion)\n",
    "    print(tl.norm(t - kruskal_to_tensor(out)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "tensor(226.9214)\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "for stop_criterion in ['rec_error_decrease', 'rec_error_deviation']:\n",
    "    (factors_als, weights_als), _ =  quantized_parafac(t, rank,\n",
    "                                                       n_iter_max=50000,\\\n",
    "                                                       init='random',\\\n",
    "                                                       tol=1e-8,\\\n",
    "                                                       svd = None,\\\n",
    "                                                       normalize_factors = normalize_factors,\\\n",
    "                                                       stop_criterion = stop_criterion\n",
    "                                                      )\n",
    "    print(tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_als, factors_als)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using quantized ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 913.5255737304688\n",
      "iteration 500, diff_from_norm = 12.325675964355469, rel_rec_error = 0.013492425739131085\n",
      "iteration 1000, diff_from_norm = 12.325695037841797, rel_rec_error = 0.013492446618115623\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "(factors_qals, weights_qals), _, scales, zero_points = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init='random', tol= False, svd = None,\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1, 2],\n",
    "                                    warmup_iters = 1,\n",
    "                                    quantize_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.3257)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_qals, factors_qals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_qals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.2144), tensor(0.1242), tensor(0.0004)),\n",
       " (tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales, zero_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.8624,  -3.6444,   8.5750,  ...,  -2.1437,   2.1437,  -2.5725],\n",
       "        [ -4.0731,  -2.5725,  -2.7869,  ...,  -8.5750,   2.1437,  12.8624],\n",
       "        [  0.4287,   0.2144,  11.3618,  ..., -15.8637,   8.3606,  -0.8575],\n",
       "        ...,\n",
       "        [ -1.2862,   0.0000,  -6.8600,  ...,  -3.8587,  -8.5750,   9.4325],\n",
       "        [  8.3606,  -0.6431, -15.0062,  ...,  -6.8600,  -1.5006,  -1.0719],\n",
       "        [ -3.6444,  -1.0719,   7.2887,  ..., -25.2961,   9.4325,  21.2230]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -60.0000,  -17.0000,   40.0000,  ...,  -10.0000,   10.0000,\n",
       "          -12.0000],\n",
       "        [ -19.0000,  -12.0000,  -13.0000,  ...,  -40.0000,   10.0000,\n",
       "           60.0000],\n",
       "        [   2.0000,    1.0000,   53.0000,  ...,  -74.0000,   39.0000,\n",
       "           -4.0000],\n",
       "        ...,\n",
       "        [  -6.0000,    0.0000,  -32.0000,  ...,  -18.0000,  -40.0000,\n",
       "           44.0000],\n",
       "        [  39.0000,   -3.0000,  -70.0000,  ...,  -32.0000,   -7.0000,\n",
       "           -5.0000],\n",
       "        [ -17.0000,   -5.0000,   34.0000,  ..., -118.0000,   44.0000,\n",
       "           99.0000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]/scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.0000,   2.0000, -21.0000,  ...,  12.0000, -38.0000,   9.0000],\n",
       "        [ -6.0000, -15.0000,  -2.0000,  ...,  60.0000,  74.0000,   9.0000],\n",
       "        [ 21.0000, -17.0000, 107.0000,  ..., -32.0000, 113.0000,  63.0000],\n",
       "        ...,\n",
       "        [ 40.0000,   0.0000, -16.0000,  ...,  14.0000,   9.0000, -46.0000],\n",
       "        [ -7.0000, 124.0000,  16.0000,  ...,  31.0000,  26.0000, -30.0000],\n",
       "        [-14.0000, -51.0000,  88.0000,  ...,   3.0000,  78.0000, -56.0000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[1]/scales[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to quantize 2 of 3 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 913.5255737304688\n",
      "iteration 500, diff_from_norm = 21.66213607788086, rel_rec_error = 0.02371267614262999\n",
      "iteration 1000, diff_from_norm = 21.66213607788086, rel_rec_error = 0.02371267614262999\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "(factors_qals, weights_qals), _, scales, zero_points = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init='random', tol= False, svd = None,\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1],\n",
    "                                    warmup_iters = 0,\n",
    "                                    quantize_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.6621)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_qals, factors_qals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
