{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, quantized_parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Tensor quantization\n",
    "Quantization scheme can be either affine or symmetric.\n",
    "\n",
    "Scale and zero_point values to perform quantization are computed either per channel or per tensor (i.e. we get either vectors or scalars).\n",
    "\n",
    "Thus, there are 4 types of quantization scheme:\n",
    "\n",
    "    ``torch.per_tensor_affine``\n",
    "    ``torch.per_tensor_symmetric``\n",
    "    ``torch.per_channel_affine``\n",
    "    ``torch.per_channel_symmetric``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_tensor|| = 767.418212890625\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(256, 256, 9)\n",
    "print('||float_tensor|| = {}'.format(tl.norm(t)))\n",
    "\n",
    "dtype = torch.qint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per channel  quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_affine\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.239077568054199\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.207361698150635\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.412856578826904\n",
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_symmetric\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.239077568054199\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.207361698150635\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.412856578826904\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_channel_affine, torch.per_channel_symmetric]:\n",
    "    print(\"\\nPer channel quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "    \n",
    "    for dim in range(len(t.shape)):\n",
    "        qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                              dtype,\\\n",
    "                                              qscheme,\\\n",
    "                                              dim = dim,\\\n",
    "                                              return_scale_zeropoint=True)\n",
    "\n",
    "        print('Per dim {}, ||float_tensor - quant_tensor|| = {}'.format(dim, tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per tensor quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_affine\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.125895500183105\n",
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_symmetric\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 8.125895500183105\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    print(\"\\nPer tensor quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "\n",
    "    \n",
    "    qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                          dtype,\\\n",
    "                                          qscheme,\\\n",
    "                                          dim = dim,\\\n",
    "                                          return_scale_zeropoint=True)\n",
    "    print('Per tensor, ||float_tensor - quant_tensor|| = {}'.format(tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2. Quantization of a tensor in Kruskal format:\n",
    "    a) via quantization of the corresponding full tensor\n",
    "    b) via quantization of decomposition factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 711.4100341796875\n"
     ]
    }
   ],
   "source": [
    "rank = 16\n",
    "shape = (64, 64, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose quantization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Quantize the full tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors - float_factors_quantized|| = 10.898009300231934\n"
     ]
    }
   ],
   "source": [
    "t_quant = quantize_qint(t, dtype, qscheme, dim = dim)\n",
    "print('||float_factors - float_factors_quantized|| = {}'.format(tl.norm(t - t_quant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Quantize several factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 8.447911262512207\n",
      "\n",
      "[2/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 14.076884269714355\n",
      "\n",
      "[3/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 35.730384826660156\n"
     ]
    }
   ],
   "source": [
    "num_factors = len(factors)\n",
    "for num_quant_factors in range(1, num_factors + 1):\n",
    "    \n",
    "    qfactors = [quantize_qint(factors[i], dtype, qscheme, dim = dim)\\\n",
    "                for i in range(num_quant_factors)\\\n",
    "               ] + [factors[i] for i in range(num_quant_factors, num_factors)]\n",
    "\n",
    "    qkrt = KruskalTensor((weights, qfactors))\n",
    "    qt = kruskal_to_tensor(qkrt)\n",
    "    print('\\n[{}/{}] factors are quantized'.format(num_quant_factors, num_factors))\n",
    "    print('||quant_factors - float_factors|| = {}'.format(tl.norm(qt - t)))\n",
    "#     print('||quant_factors - float_factors_quantized|| = {}'.format(tl.norm(qt - t_quant)))\n",
    "\n",
    "#     qt_quant = quantize_qint(qt, dtype, qscheme, dim = dim)\n",
    "#     print('\\nquant_factors_quantized - t_quant_factors: {}'.format(tl.norm(qt_quant - qt)/tnorm))\n",
    "    \n",
    "#     print('||quant_factors_quantized - float_factors|| = {}'.format(tl.norm(qt_quant - t)/tnorm))\n",
    "#     print('||quant_factors_quantized - float_factors_quantized|| = {}'.format(tl.norm(qt_quant - t_quant)/tnorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3. Quantized ALS\n",
    "Compare standard ALS algorithm for finding  CP decomposition with its quantized version, when at the end of each ALS step approximated factor is quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 1053.6529541015625\n"
     ]
    }
   ],
   "source": [
    "rank = 8\n",
    "shape = (128, 128, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 1053.6529541015625\n",
      "parafac has stopped after iteration 59\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "(factors_als, weights_als), _ =  quantized_parafac(t, rank, n_iter_max=50000,\\\n",
    "                                    init='random', tol=1e-8, svd = None,\n",
    "                                    normalize_factors = normalize_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0003)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_als, factors_als))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using quantized ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 1053.6529541015625\n",
      "iteration 500, diff_from_norm = 14.47594928741455, rel_rec_error = 0.013738820957188909\n",
      "iteration 1000, diff_from_norm = 14.475943565368652, rel_rec_error = 0.013738815526514724\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "(factors_qals, weights_qals), _, scales, zero_points = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init='random', tol= False, svd = None,\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1, 2],\n",
    "                                    warmup_iters = 60,\n",
    "                                    quantize_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.4759)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_qals, factors_qals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_qals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.0665), tensor(0.0308), tensor(0.0085)),\n",
       " (tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales, zero_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1302,  1.6621, -4.0555,  ..., -1.7286, -0.8643, -1.9280],\n",
       "        [ 3.3907, -0.9973,  1.7951,  ..., -3.1912,  3.5901, -2.5928],\n",
       "        [-2.4599, -0.9308, -0.1330,  ..., -0.5983, -1.0637, -0.1995],\n",
       "        ...,\n",
       "        [ 2.1939, -1.0637,  2.7923,  ..., -1.4626,  0.0665, -5.3187],\n",
       "        [ 1.1967, -0.3989, -3.5901,  ..., -2.5264, -1.1302,  1.7951],\n",
       "        [ 1.0637,  0.3324, -3.3242,  ...,  1.3297,  2.9253,  1.3297]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.,  25., -61.,  ..., -26., -13., -29.],\n",
       "        [ 51., -15.,  27.,  ..., -48.,  54., -39.],\n",
       "        [-37., -14.,  -2.,  ...,  -9., -16.,  -3.],\n",
       "        ...,\n",
       "        [ 33., -16.,  42.,  ..., -22.,   1., -80.],\n",
       "        [ 18.,  -6., -54.,  ..., -38., -17.,  27.],\n",
       "        [ 16.,   5., -50.,  ...,  20.,  44.,  20.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[0]/scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 90.,   2.,   0.,  ...,  50.,  54.,   7.],\n",
       "        [  7.,  16.,  13.,  ...,   5., -32.,  41.],\n",
       "        [  3.,  13., -23.,  ...,  17.,  -6., -10.],\n",
       "        ...,\n",
       "        [ 90.,   2.,  -8.,  ...,  23.,  10., -28.],\n",
       "        [-65., -41.,  13.,  ...,  44.,  31.,  -6.],\n",
       "        [-31.,  17.,  -5.,  ...,  48., -26.,   3.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_qals[1]/scales[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to quantize 2 of 3 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac original_tensor_norm = 1053.6529541015625\n",
      "iteration 500, diff_from_norm = 16.254718780517578, rel_rec_error = 0.015427013911214994\n",
      "iteration 1000, diff_from_norm = 16.254718780517578, rel_rec_error = 0.015427013911214994\n"
     ]
    }
   ],
   "source": [
    "normalize_factors = False\n",
    "(factors_qals, weights_qals), _, scales, zero_points = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init='random', tol= False, svd = None,\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1],\n",
    "                                    warmup_iters = 60,\n",
    "                                    quantize_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.2547)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(KruskalTensor((weights_qals, factors_qals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
