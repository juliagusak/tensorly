{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, quantized_parafac\n",
    "from tensorly.kruskal_tensor import kruskal_to_tensor, KruskalTensor\n",
    "from tensorly.base import unfold\n",
    "from tensorly.quantization import quantize_qint\n",
    "\n",
    "import torch\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Tensor quantization\n",
    "We use PyTorch build-in tools to perform quantization.\n",
    "\n",
    "Quantization scheme can be either affine or symmetric.\n",
    "\n",
    "Scale and zero_point values to perform quantization are computed either per channel or per tensor (i.e. we get either vectors or scalars).\n",
    "\n",
    "Thus, there are 4 types of quantization scheme:\n",
    "\n",
    "    ``torch.per_tensor_affine``\n",
    "    ``torch.per_tensor_symmetric``\n",
    "    ``torch.per_channel_affine``\n",
    "    ``torch.per_channel_symmetric``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_tensor|| = 767.9274291992188\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(256, 256, 9)\n",
    "print('||float_tensor|| = {}'.format(tl.norm(t)))\n",
    "\n",
    "dtype = torch.qint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per channel  quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_affine\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.307478427886963\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.122038841247559\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.646827220916748\n",
      "\n",
      "Per channel quantization, dtype: torch.qint8, qscheme: torch.per_channel_symmetric\n",
      "Per dim 0, ||float_tensor - quant_tensor|| = 7.307478427886963\n",
      "Per dim 1, ||float_tensor - quant_tensor|| = 7.122038841247559\n",
      "Per dim 2, ||float_tensor - quant_tensor|| = 7.646827220916748\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_channel_affine, torch.per_channel_symmetric]:\n",
    "    print(\"\\nPer channel quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "    \n",
    "    for dim in range(len(t.shape)):\n",
    "        qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                              dtype,\\\n",
    "                                              qscheme,\\\n",
    "                                              dim = dim,\\\n",
    "                                              return_scale_zeropoint=True)\n",
    "\n",
    "        print('Per dim {}, ||float_tensor - quant_tensor|| = {}'.format(dim, tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per tensor quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_affine\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 9.011683464050293\n",
      "\n",
      "Per tensor quantization, dtype: torch.qint8, qscheme: torch.per_tensor_symmetric\n",
      "Per tensor, ||float_tensor - quant_tensor|| = 9.011683464050293\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    print(\"\\nPer tensor quantization, dtype: {}, qscheme: {}\".format(dtype, qscheme))\n",
    "\n",
    "    \n",
    "    qt, scale, zero_point = quantize_qint(t,\\\n",
    "                                          dtype,\\\n",
    "                                          qscheme,\\\n",
    "                                          dim = dim,\\\n",
    "                                          return_scale_zeropoint=True)\n",
    "    print('Per tensor, ||float_tensor - quant_tensor|| = {}'.format(tl.norm(t - qt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2. Quantization of a tensor in Kruskal format:\n",
    "    a) via quantization of the corresponding full tensor\n",
    "    b) via quantization of decomposition factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 713.4921875\n"
     ]
    }
   ],
   "source": [
    "rank = 16\n",
    "shape = (64, 64, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose quantization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Quantize the full tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors - float_factors_quantized|| = 10.873172760009766\n"
     ]
    }
   ],
   "source": [
    "t_quant = quantize_qint(t, dtype, qscheme, dim = dim)\n",
    "print('||float_factors - float_factors_quantized|| = {}'.format(tl.norm(t - t_quant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Quantize several factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 12.959274291992188\n",
      "\n",
      "[2/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 14.07798957824707\n",
      "\n",
      "[3/3] factors are quantized\n",
      "||quant_factors - float_factors|| = 25.923023223876953\n"
     ]
    }
   ],
   "source": [
    "num_factors = len(factors)\n",
    "for num_quant_factors in range(1, num_factors + 1):\n",
    "    \n",
    "    qfactors = [quantize_qint(factors[i], dtype, qscheme, dim = dim)\\\n",
    "                for i in range(num_quant_factors)\\\n",
    "               ] + [factors[i] for i in range(num_quant_factors, num_factors)]\n",
    "\n",
    "    qkrt = KruskalTensor((weights, qfactors))\n",
    "    qt = kruskal_to_tensor(qkrt)\n",
    "    print('\\n[{}/{}] factors are quantized'.format(num_quant_factors, num_factors))\n",
    "    print('||quant_factors - float_factors|| = {}'.format(tl.norm(qt - t)))\n",
    "#     print('||quant_factors - float_factors_quantized|| = {}'.format(tl.norm(qt - t_quant)))\n",
    "\n",
    "#     qt_quant = quantize_qint(qt, dtype, qscheme, dim = dim)\n",
    "#     print('\\nquant_factors_quantized - t_quant_factors: {}'.format(tl.norm(qt_quant - qt)/tnorm))\n",
    "    \n",
    "#     print('||quant_factors_quantized - float_factors|| = {}'.format(tl.norm(qt_quant - t)/tnorm))\n",
    "#     print('||quant_factors_quantized - float_factors_quantized|| = {}'.format(tl.norm(qt_quant - t_quant)/tnorm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3. Quantized ALS\n",
    "Compare standard ALS algorithm for finding  CP decomposition with its quantized version, when at the end of each ALS step approximated factor is quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 1578.495849609375\n",
      "\n",
      "=============== random state 8525 ==================\n",
      "||float_ffactors||: 0.0004248009354341775\n",
      "||float_qqfactors||: 38.49079513549805\n",
      "||float_qfactors||: 26.33399772644043\n",
      "\n",
      "=============== random state 9858 ==================\n",
      "||float_ffactors||: 0.0004599247477017343\n",
      "||float_qqfactors||: 27.231096267700195\n",
      "||float_qfactors||: 290.12408447265625\n",
      "\n",
      "=============== random state 8794 ==================\n",
      "||float_ffactors||: 0.0003965498472098261\n",
      "||float_qqfactors||: 48.92815017700195\n",
      "||float_qfactors||: 27.728422164916992\n",
      "\n",
      "=============== random state 4963 ==================\n",
      "||float_ffactors||: 0.00039850908797234297\n",
      "||float_qqfactors||: 41.202423095703125\n",
      "||float_qfactors||: 19.78460121154785\n",
      "\n",
      "=============== random state 9217 ==================\n",
      "||float_ffactors||: 0.0004433225258253515\n",
      "||float_qqfactors||: 35.17451858520508\n",
      "||float_qfactors||: 289.84814453125\n",
      "\n",
      "=============== random state 8863 ==================\n",
      "||float_ffactors||: 0.00046504897181876004\n",
      "||float_qqfactors||: 28.925840377807617\n",
      "||float_qfactors||: 27.65139389038086\n",
      "\n",
      "=============== random state 8849 ==================\n",
      "||float_ffactors||: 288.95501708984375\n",
      "||float_qqfactors||: 290.4665832519531\n",
      "||float_qfactors||: 20.928878784179688\n",
      "\n",
      "=============== random state 2454 ==================\n",
      "||float_ffactors||: 0.0004377279838081449\n",
      "||float_qqfactors||: 52.634944915771484\n",
      "||float_qfactors||: 293.1118469238281\n",
      "\n",
      "=============== random state 9019 ==================\n",
      "||float_ffactors||: 292.2660217285156\n",
      "||float_qqfactors||: 296.01776123046875\n",
      "||float_qfactors||: 22.964323043823242\n",
      "\n",
      "=============== random state 4222 ==================\n",
      "||float_ffactors||: 0.0004785775672644377\n",
      "||float_qqfactors||: 33.589271545410156\n",
      "||float_qfactors||: 290.05694580078125\n"
     ]
    }
   ],
   "source": [
    "### All in one cell\n",
    "rank = 12\n",
    "shape = (128, 128, 9)\n",
    "stop_criterion = 'rec_error_decrease'\n",
    "init = 'random'\n",
    "niter_max = 1000\n",
    "\n",
    "r_float = 0\n",
    "\n",
    "## Quantization parameters\n",
    "dtype = torch.qint8\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Generate tensor\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t_original = kruskal_to_tensor(krt)\n",
    "t = t_original\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))\n",
    "\n",
    "\n",
    "## Perform als, post als factors quantization,  quantized als\n",
    "for _ in range(10):\n",
    "    random_state = int(torch.randint(high = 11999, size = (1,)))\n",
    "    print('\\n=============== random state {} =================='.format(random_state))\n",
    "\n",
    "    ## ALS with float factors\n",
    "    t = t_original\n",
    "    r = rank - r_float\n",
    "    out = parafac(t, r,\\\n",
    "                  n_iter_max=niter_max, init=init, svd='numpy_svd',\\\n",
    "                  normalize_factors=False, orthogonalise=False,\\\n",
    "                  tol=None, random_state=random_state,\\\n",
    "                  verbose=0, return_errors=False,\\\n",
    "                  non_negative=False, mask=None,\n",
    "                  stop_criterion = stop_criterion)\n",
    "\n",
    "    t_approx_als = kruskal_to_tensor(out) \n",
    "    print('||float_ffactors||: {}'.format(tl.norm(t - t_approx_als)))\n",
    "\n",
    "    ## Post ALS factors quantization\n",
    "    qqfactors = [quantize_qint(out.factors[i], dtype, qscheme, dim = dim)\\\n",
    "                    for i in range(len(out.factors))]\n",
    "    qqkrt = KruskalTensor((out.weights, qqfactors))\n",
    "    # corresponding tensor in full format\n",
    "    qqt = kruskal_to_tensor(qqkrt)\n",
    "    print('||float_qqfactors||: {}'.format(tl.norm(t - qqt)))\n",
    "\n",
    "\n",
    "    ## Quantized ALS\n",
    "    t = t_original\n",
    "    r = rank - r_float\n",
    "    normalize_factors = False\n",
    "    (fout, qout), (errors, qerrors), scales, zero_points = quantized_parafac(\n",
    "                                        t, r, n_iter_max=1000,\\\n",
    "                                        init=init, tol= None, svd = 'numpy_svd',\\\n",
    "                                        normalize_factors = normalize_factors,\\\n",
    "                                        qmodes = [0, 1, 2],\n",
    "                                        warmup_iters = 1,\n",
    "                                        freeze_every = 1,\n",
    "                                        qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                        return_scale_zeropoint=True, verbose = False,\\\n",
    "                                        random_state=random_state,\\\n",
    "                                        return_rec_errors=False, return_qrec_errors=False,\n",
    "                                        stop_criterion = stop_criterion)\n",
    "\n",
    "    t_approx_qals = kruskal_to_tensor(KruskalTensor(qout))\n",
    "    print('||float_qfactors||: {}'.format(tl.norm(t - t_approx_qals)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_factors||: 713.4921875\n"
     ]
    }
   ],
   "source": [
    "rank = 12\n",
    "shape = (128, 128, 9)\n",
    "\n",
    "factors = [torch.randn((i, rank)) for i in shape] \n",
    "weights = torch.ones(rank)\n",
    "\n",
    "# tensor in Kruscal format\n",
    "krt = KruskalTensor((weights, factors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "t_original = kruskal_to_tensor(krt)\n",
    "\n",
    "tnorm = tl.norm(t)\n",
    "print('||float_factors||: {}'.format(tnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find rank-1 float approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_float = 0\n",
    "stop_criterion = 'rec_error_decrease'\n",
    "qstop_criterion = 'rec_error_decrease'\n",
    "init = 'svd'\n",
    "random_state = 1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_original\n",
    "r = r_float\n",
    "if r > 0:\n",
    "    out = parafac(t, r,\\\n",
    "                  n_iter_max=1000, init=init, svd='numpy_svd',\\\n",
    "                  normalize_factors=False, orthogonalise=False,\\\n",
    "                  tol=None, random_state=random_state,\\\n",
    "                  verbose=0, return_errors=False,\\\n",
    "                  non_negative=False, mask=None,\n",
    "                  stop_criterion = stop_criterion)\n",
    "\n",
    "    t_approx = kruskal_to_tensor(out)\n",
    "    print(tl.norm(t - t_approx)) \n",
    "else:\n",
    "    t_approx = 0\n",
    "    \n",
    "t_residual = t - t_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004)\n",
      "tensor(0.0004)\n"
     ]
    }
   ],
   "source": [
    "t = t_residual\n",
    "r = rank - r_float\n",
    "out = parafac(t, r,\\\n",
    "              n_iter_max=1000, init=init, svd='numpy_svd',\\\n",
    "              normalize_factors=False, orthogonalise=False,\\\n",
    "              tol=None, random_state=random_state,\\\n",
    "              verbose=0, return_errors=False,\\\n",
    "              non_negative=False, mask=None,\n",
    "              stop_criterion = stop_criterion)\n",
    "\n",
    "t_residual_approx_als = kruskal_to_tensor(out) \n",
    "print(tl.norm(t - t_residual_approx_als))  \n",
    "print(tl.norm(t - t_approx - t_residual_approx_als))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find an approximation using quantized ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.qint8\n",
    "\n",
    "## Per tensor quantization\n",
    "qscheme, dim = torch.per_tensor_affine, None\n",
    "\n",
    "## Uncomment for per channel quantization\n",
    "# qscheme, dim = torch.per_channel_affine, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parafac norm_tensor = 1322.6064453125\n",
      "float factors reconstruction error=0.7177299857139587\n",
      "quantized factors reconstruction error=0.9999871709581504\n",
      "iteration 100, float factors reconstraction error: 0.03975190967321396, decrease = -0.003070201724767685, unnormalized = 52.576133728027344\n",
      "iteration 100, quantized factors reconstraction error: 0.02207577133229692, decrease = -2.884227034566367e-07, unnormalized = 29.19755744934082\n",
      "iteration 200, float factors reconstraction error: 0.03975910320878029, decrease = -0.003073502331972122, unnormalized = 52.58564376831055\n",
      "iteration 200, quantized factors reconstraction error: 0.022075778542864504, decrease = -2.985174980749128e-07, unnormalized = 29.197566986083984\n",
      "iteration 300, float factors reconstraction error: 0.03971414640545845, decrease = -0.0029993392527103424, unnormalized = 52.52618408203125\n",
      "iteration 300, quantized factors reconstraction error: 0.02207578719554561, decrease = -3.187070873184039e-07, unnormalized = 29.19757843017578\n",
      "iteration 400, float factors reconstraction error: 0.03976628929376602, decrease = -0.0031060203909873962, unnormalized = 52.595149993896484\n",
      "iteration 400, quantized factors reconstraction error: 0.02207578719554561, decrease = -3.172649737995248e-07, unnormalized = 29.19757843017578\n",
      "iteration 500, float factors reconstraction error: 0.03976988419890404, decrease = -0.0031154602766036987, unnormalized = 52.59990310668945\n",
      "iteration 500, quantized factors reconstraction error: 0.022075817479929472, decrease = -3.590862658019167e-07, unnormalized = 29.19761848449707\n",
      "iteration 600, float factors reconstraction error: 0.039782460778951645, decrease = -0.0031319372355937958, unnormalized = 52.616539001464844\n",
      "iteration 600, quantized factors reconstraction error: 0.022075847764313334, decrease = -4.023496713162489e-07, unnormalized = 29.19765853881836\n",
      "iteration 700, float factors reconstraction error: 0.03976628929376602, decrease = -0.0031060203909873962, unnormalized = 52.595149993896484\n",
      "iteration 700, quantized factors reconstraction error: 0.022075886701378302, decrease = -4.5426575794038637e-07, unnormalized = 29.197710037231445\n",
      "iteration 800, float factors reconstraction error: 0.03975910320878029, decrease = -0.003077395260334015, unnormalized = 52.58564376831055\n",
      "iteration 800, quantized factors reconstraction error: 0.02207588237503775, decrease = -4.5138153090609756e-07, unnormalized = 29.197704315185547\n",
      "iteration 900, float factors reconstraction error: 0.03978605195879936, decrease = -0.003147229552268982, unnormalized = 52.62128829956055\n",
      "iteration 900, quantized factors reconstraction error: 0.022075893911945886, decrease = -4.672447795929513e-07, unnormalized = 29.19771957397461\n",
      "tensor(29.1971)\n",
      "tensor(29.1971)\n"
     ]
    }
   ],
   "source": [
    "t = t_residual\n",
    "r = rank - r_float\n",
    "normalize_factors = False\n",
    "(fout, qout), (errors, qerrors), scales, zero_points = quantized_parafac(\n",
    "                                    t, r, n_iter_max=1000,\\\n",
    "                                    init=init, tol= None, svd = 'numpy_svd',\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 1, 2],\n",
    "                                    warmup_iters = 1,\n",
    "                                    freeze_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=True,\\\n",
    "                                    return_rec_errors = True,\\\n",
    "                                    return_qrec_errors=True,\\\n",
    "                                    verbose = True,\\\n",
    "                                    random_state=random_state,\n",
    "                                    stop_criterion = stop_criterion)\n",
    "\n",
    "t_residual_approx_qals = kruskal_to_tensor(qout)\n",
    "print(tl.norm(t_residual - t_residual_approx_qals))\n",
    "print(tl.norm(t - t_approx - t_residual_approx_qals))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7870),\n",
       " tensor(0.7881),\n",
       " tensor(0.6288),\n",
       " tensor(0.5085),\n",
       " tensor(0.4267),\n",
       " tensor(0.3649),\n",
       " tensor(0.3428),\n",
       " tensor(0.3297),\n",
       " tensor(0.2887),\n",
       " tensor(0.2595),\n",
       " tensor(0.2317),\n",
       " tensor(0.2136),\n",
       " tensor(0.2111),\n",
       " tensor(0.1964),\n",
       " tensor(0.1941),\n",
       " tensor(0.1880),\n",
       " tensor(0.1836),\n",
       " tensor(0.1848),\n",
       " tensor(0.1868),\n",
       " tensor(0.1859),\n",
       " tensor(0.1835),\n",
       " tensor(0.1832),\n",
       " tensor(0.1858),\n",
       " tensor(0.1853),\n",
       " tensor(0.1888),\n",
       " tensor(0.1869),\n",
       " tensor(0.1862),\n",
       " tensor(0.1862),\n",
       " tensor(0.1861),\n",
       " tensor(0.1867),\n",
       " tensor(0.1837),\n",
       " tensor(0.1862),\n",
       " tensor(0.1861),\n",
       " tensor(0.1853),\n",
       " tensor(0.1874),\n",
       " tensor(0.1857),\n",
       " tensor(0.1863),\n",
       " tensor(0.1858),\n",
       " tensor(0.1880),\n",
       " tensor(0.1858),\n",
       " tensor(0.1870),\n",
       " tensor(0.1872),\n",
       " tensor(0.1860),\n",
       " tensor(0.1871),\n",
       " tensor(0.1872),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1868),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1869),\n",
       " tensor(0.1871),\n",
       " tensor(0.1868),\n",
       " tensor(0.1861),\n",
       " tensor(0.1854),\n",
       " tensor(0.1812),\n",
       " tensor(0.1833),\n",
       " tensor(0.1832),\n",
       " tensor(0.1832),\n",
       " tensor(0.1825),\n",
       " tensor(0.1822),\n",
       " tensor(0.1816),\n",
       " tensor(0.1817),\n",
       " tensor(0.1828),\n",
       " tensor(0.1826),\n",
       " tensor(0.1831),\n",
       " tensor(0.1832),\n",
       " tensor(0.1826),\n",
       " tensor(0.1829),\n",
       " tensor(0.1826),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828),\n",
       " tensor(0.1825),\n",
       " tensor(0.1828))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0129274650840132,\n",
       " 0.7892980153660368,\n",
       " 0.629156786677094,\n",
       " 0.5096062751091962,\n",
       " 0.42674935520532004,\n",
       " 0.36507604699952506,\n",
       " 0.34239840430556934,\n",
       " 0.32995667627454206,\n",
       " 0.28811078922120226,\n",
       " 0.2600321850128048,\n",
       " 0.2321932555660405,\n",
       " 0.2162646889789725,\n",
       " 0.20802160221866167,\n",
       " 0.20069085405829057,\n",
       " 0.1940647277272738,\n",
       " 0.18861278802978557,\n",
       " 0.18535246266936414,\n",
       " 0.1845577644654968,\n",
       " 0.18442594538084364,\n",
       " 0.18434355845293543,\n",
       " 0.18430631142454038,\n",
       " 0.1842818801404866,\n",
       " 0.18426371433144412,\n",
       " 0.18426446700214352,\n",
       " 0.1842547229679539,\n",
       " 0.1842535227633251,\n",
       " 0.1842383269521776,\n",
       " 0.18423346510630842,\n",
       " 0.18421869648663897,\n",
       " 0.1842343601741672,\n",
       " 0.18422268360710067,\n",
       " 0.18421257340878702,\n",
       " 0.18422386346927813,\n",
       " 0.184219184705471,\n",
       " 0.1842086473156793,\n",
       " 0.18420777259027188,\n",
       " 0.18421236998427365,\n",
       " 0.1842139363530265,\n",
       " 0.18421251238143302,\n",
       " 0.18421499416049592,\n",
       " 0.18420992889011342,\n",
       " 0.18420998991746743,\n",
       " 0.18421220724466297,\n",
       " 0.18420651135828908,\n",
       " 0.18421224792956564,\n",
       " 0.18420921690431669,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.18421011197217543,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.18421011197217543,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420942032883003,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.1842100916297241,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421013231462677,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420935930147603,\n",
       " 0.18421011197217543,\n",
       " 0.18420937964392736,\n",
       " 0.18421011197217543,\n",
       " 0.1842093999863787,\n",
       " 0.18421057984855616,\n",
       " 0.18420842354871458,\n",
       " 0.18420708094692648,\n",
       " 0.18420443642825288,\n",
       " 0.18420362273019944,\n",
       " 0.18420138506055256,\n",
       " 0.1841981302683389,\n",
       " 0.18419644184487807,\n",
       " 0.1841960553383027,\n",
       " 0.18419265814892968,\n",
       " 0.18419391938091248,\n",
       " 0.18419798787117955,\n",
       " 0.18419621807791337,\n",
       " 0.18419920841825968,\n",
       " 0.18419943218522436,\n",
       " 0.18419402109316915,\n",
       " 0.18419636047507273,\n",
       " 0.18419326842246975,\n",
       " 0.18419385835355848,\n",
       " 0.18419298362815106,\n",
       " 0.1841927191762837,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419275986118636,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419275986118636,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419275986118636,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419296328569973,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.1841927802036377,\n",
       " 0.1841929429432484,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.18419286157344306,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419282088854036,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.1841928819158944,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.1841928819158944,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.18419280054608902,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.1841927802036377,\n",
       " 0.18419292260079706,\n",
       " 0.18419280054608902,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573,\n",
       " 0.1841927802036377,\n",
       " 0.18419290225834573)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||float_qqfactors||: 44.78242111206055\n"
     ]
    }
   ],
   "source": [
    "qqfactors = [quantize_qint(out.factors[i], dtype, qscheme, dim = dim)\\\n",
    "                for i in range(len(out.factors))]\n",
    "\n",
    "qqkrt = KruskalTensor((out.weights, qqfactors))\n",
    "\n",
    "# corresponding tensor in full format\n",
    "qqt = kruskal_to_tensor(qqkrt)\n",
    "\n",
    "print('||float_qqfactors||: {}'.format(tl.norm(t - qqt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.1900), tensor(0.0138), tensor(0.0146)),\n",
       " (tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32),\n",
       "  tensor(0, dtype=torch.int32)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales, zero_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.3201,  11.7802,  -2.6600,  ...,   2.2800,  -0.3800,   0.0000],\n",
       "        [ -2.2800,   2.4700,   1.3300,  ...,  -2.2800,  -0.5700,  -1.9000],\n",
       "        [ -7.0301,  -6.6501,  -8.1701,  ...,  -2.6600,   1.1400,   0.3800],\n",
       "        ...,\n",
       "        [  5.5101,  -1.5200,  -0.7600,  ...,   9.8802,  -4.7501,  -0.3800],\n",
       "        [  7.2201, -13.3002,   5.5101,  ...,  -0.9500,   2.4700,   0.7600],\n",
       "        [ -1.1400,  -2.4700,  10.2602,  ...,  -4.1801,   2.2800,   0.3800]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qout.factors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28.0000,  62.0000, -14.0000,  ...,  12.0000,  -2.0000,   0.0000],\n",
       "        [-12.0000,  13.0000,   7.0000,  ..., -12.0000,  -3.0000, -10.0000],\n",
       "        [-37.0000, -35.0000, -43.0000,  ..., -14.0000,   6.0000,   2.0000],\n",
       "        ...,\n",
       "        [ 29.0000,  -8.0000,  -4.0000,  ...,  52.0000, -25.0000,  -2.0000],\n",
       "        [ 38.0000, -70.0000,  29.0000,  ...,  -5.0000,  13.0000,   4.0000],\n",
       "        [ -6.0000, -13.0000,  54.0000,  ..., -22.0000,  12.0000,   2.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qout.factors[0]/scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.0000, -17.0000, -15.0000,  ...,  11.0000, -27.0000,  19.0000],\n",
       "        [ -9.0000,   3.0000,  20.0000,  ..., -38.0000,   1.0000, -24.0000],\n",
       "        [-21.0000,  39.0000,   0.0000,  ..., -11.0000, -24.0000, -38.0000],\n",
       "        ...,\n",
       "        [-15.0000,  -5.0000, -36.0000,  ...,  29.0000,  63.0000,  -2.0000],\n",
       "        [-37.0000, -38.0000,  10.0000,  ..., -29.0000,  -6.0000,  41.0000],\n",
       "        [-10.0000,  14.0000, -15.0000,  ...,   5.0000,   8.0000,   5.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qout.factors[1]/scales[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to quantize 2 of 3 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_factors = False\n",
    "(ft, qt), _ = quantized_parafac(\n",
    "                                    t, rank, n_iter_max=1001,\\\n",
    "                                    init=init, tol= False, svd = 'numpy_svd',\\\n",
    "                                    normalize_factors = normalize_factors,\\\n",
    "                                    qmodes = [0, 2],\n",
    "                                    warmup_iters = 1,\n",
    "                                    freeze_every = 1,\n",
    "                                    qscheme = qscheme, dtype = dtype, dim = dim,\n",
    "                                    return_scale_zeropoint=False, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.5654)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(t - kruskal_to_tensor(qt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
